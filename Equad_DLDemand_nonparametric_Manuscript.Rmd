---
title: "A Nonparametric Demand Analysis"
subtitle: "Recovering elasticities with nonparametric regression"
author: "Matthew Aaron Looney"
date: "08/22/2017"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: no
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '4'
header-includes:
- \usepackage{graphicx}
- \usepackage{rotating}
- \usepackage{longtable}
- \usepackage{amssymb,amsmath}
- \usepackage{dcolumn}
- \usepackage{setspace}\doublespacing
- \usepackage{mathrsfs}
- \usepackage{eso-pic,graphicx,transparent}

abstract: "Nonparametric econometric analysis has been a growing field of study over the past several decades. Many new techniques have been developed within a theoretical framework. However, despite the rapid growth of theoretical results, nonparametric applied research has lagged considerably. This paper employs a nonparametric regression analysis within the context of demand theory. Data on prices and quantites of three commodities (meats, dairy products and beans) from the 2006 Ecuadorian consumer expenditure survey will be evaluated to derive Marshallian price and income elasticities. The nonparametric results will be compared with standard parametric demand analysis tools such as the Log-Log demand model and the Almost Ideal Demand system to guage the effectivness of using nonparametric techniques to estimate demand elasticities."

bibliography: np_demand_paper.bib
csl: computational-economics.csl
link-citations: true
fontsize: 12pt
geometry: margin = 1.25in
---

\AddToShipoutPictureFG{
  \AtPageCenter{% or \AtTextCenter
    \makebox[0pt]{\rotatebox[origin=c]{45}{%
      \scalebox{5}{\texttransparent{0.05}{DRAFT}}%
    }}
  }
}

```{r House Keeping, echo=FALSE, message=FALSE, cache=T}

rm(list=ls())

# randNum <- round(runif(1, 1, 1e8)) # !!! RUN ONLY ONCE TO GENERATE RN for seed !!!
# randNum
set.seed(38532842) # all sequential simulations will start from this seed

library(readxl)
library(systemfit)
library(np)
library(micEconAids)
library(stargazer)

options(np.messages=FALSE) 
#options(digits=5)

setwd("~/Google Drive/digitalLibrary/*AAEC 6310 Demand and Price Analysis/DLDemand_nonparametric")

source("~/Google Drive/digitalLibrary/*AAEC 6310 Demand and Price Analysis/demandLab1/calculate_pValue.R")
source("~/Google Drive/digitalLibrary/*AAEC 6310 Demand and Price Analysis/demandLab1/calculate_tValues.R")

```

```{r Import & data manip, echo=FALSE, message=FALSE, cache=T}

Dta <- read.csv("/Users/malooney/Google Drive/digitalLibrary/*Econometrics2/Lab3/protein.csv")
Dta_sub <- data.frame(obs=1:nrow(Dta))

Dta_sub$p1 <- Dta$pmeats1
Dta_sub$p2 <- Dta$pdairy1
Dta_sub$p3 <- Dta$ppulses1

Dta_sub$q1 <- Dta$qmeats
Dta_sub$q2 <- Dta$qdairy
Dta_sub$q3 <- Dta$qpulses

Dta_sub$p1q1 <- Dta$qmeats*Dta$pmeats1
Dta_sub$p2q2 <- Dta$qdairy*Dta$pdairy1
Dta_sub$p3q3 <- Dta$ppulses1*Dta$qpulses

Dta_sub$X <- Dta_sub$p1q1 + Dta_sub$p2q2 + Dta_sub$p3q3

Dta_sub$w1 <- Dta_sub$p1q1 / Dta_sub$X
Dta_sub$w2 <- Dta_sub$p2q2 / Dta_sub$X
Dta_sub$w3 <- Dta_sub$p3q3 / Dta_sub$X

Dta_sub_log <- Dta_sub
Dta_sub_log <- log(Dta_sub[,c(2:7,11)])

```

```{r Single Equation Model Estimation using Log-Log Functional Form, echo=FALSE, message=FALSE, cache=T}

# Single Equation Model Estimation using Log-Log Functional Form
# Estimate Marshallian elasticities - unrestricted

dld_marshallian_1.lm <- lm(q1 ~ p1+ p2+ p3+ X, data=Dta_sub_log)
dld_marshallian_2.lm <- lm(q2 ~ p1+ p2+ p3+ X, data=Dta_sub_log)
dld_marshallian_3.lm <- lm(q3 ~ p1+ p2+ p3+ X, data=Dta_sub_log)

smry_dld_marshallian_1 <- summary(dld_marshallian_1.lm)
smry_dld_marshallian_2 <- summary(dld_marshallian_2.lm)
smry_dld_marshallian_3 <- summary(dld_marshallian_3.lm)

exx_par <- matrix(unlist(c(coef(dld_marshallian_1.lm)[2:5], coef(dld_marshallian_2.lm)[2:5], coef(dld_marshallian_3.lm)[2:5])), nrow = 3, byrow = TRUE)

exx_par_t <- matrix(
  unlist(c( 
  coef(dld_marshallian_1.lm)[2:5]/sqrt(diag(vcov(dld_marshallian_1.lm)[-1,-1])), 
  coef(dld_marshallian_2.lm)[2:5]/sqrt(diag(vcov(dld_marshallian_2.lm)[-1,-1])), 
  coef(dld_marshallian_3.lm)[2:5]/sqrt(diag(vcov(dld_marshallian_3.lm)[-1,-1]))
  )), 
  nrow = 3, byrow = TRUE)

######################################################################################
### DLD_parametric - calculate signif on Marshallian demand elasticities matrix    ###
######################################################################################
i <- 1
j <- 1
temp <- matrix(nrow=3, ncol = 4, 
  dimnames = list(c("meats_lnq1", "dairy_lnq2", "beans_lnq3"), c("meats", "dairy", "beans", "income_elasticity"))
  )

while(i != nrow(exx_par_t)+1 && j != ncol(exx_par_t) ){
  
  for(j in 1:ncol(exx_par_t)){
    temp[i,j] <- paste(signif(exx_par[i,j], 4), calculate_pValue(exx_par_t[i,j], 156)[2], sep="")
    j <- j+1
  }
  i <- i+1
  j <- 1
}
exx_par_t_sig <- temp # put results into matrix

exx_par_r2 <- matrix(c(signif(smry_dld_marshallian_1$r.squared, 5), signif(smry_dld_marshallian_2$r.squared, 5), signif(smry_dld_marshallian_3$r.squared, 5)), ncol=1)

exx_par_wr2 <- cbind(exx_par_t_sig, exx_par_r2)
dimnames(exx_par_wr2) <- list(c("meats_lnq1", "dairy_lnq2", "beans_lnq3"), c("meats", "dairy", "beans", "income_elasticity", "R-squared"))

```
  
```{r, echo=FALSE, message=FALSE, fig.height=3, cache=T}

dld_1_formula <- formula(q1 ~ p1+ p2+ p3+ X)
dld_2_formula <- formula(q2 ~ p1+ p2+ p3+ X)
dld_3_formula <- formula(q3 ~ p1+ p2+ p3+ X)

xdat <- Dta_sub_log[,c(1:3, 7)]
ydat_1 <- Dta_sub_log$q1
ydat_2 <- Dta_sub_log$q2
ydat_3 <- Dta_sub_log$q3

dld_1 <- lm(q1 ~ p1+ p2+ p3+ X, data=Dta_sub_log, x=TRUE, y=TRUE)
dld_2 <- lm(q2 ~ p1+ p2+ p3+ X, data=Dta_sub_log, x=TRUE, y=TRUE)
dld_3 <- lm(q3 ~ p1+ p2+ p3+ X, data=Dta_sub_log, x=TRUE, y=TRUE)

### !!! time consuming code - only run when chaging code !!! ###
### Consistent Model Specification Test
# cmsTest_1 <- npcmstest(model=dld_1, xdat=xdat, ydat = ydat_1)
# cmsTest_2 <- npcmstest(model=dld_2, xdat=xdat, ydat = ydat_2)
# cmsTest_3 <- npcmstest(model=dld_3, xdat=xdat, ydat = ydat_3)
# 
# saveRDS(cmsTest_1, file= "Equaddata/cmsTest_1.rds")
# saveRDS(cmsTest_2, file= "Equaddata/cmsTest_2.rds")
# saveRDS(cmsTest_3, file= "Equaddata/cmsTest_3.rds")

cmsTest_1 <- readRDS("Equaddata/cmsTest_1.rds")
cmsTest_2 <- readRDS("Equaddata/cmsTest_2.rds")
cmsTest_3 <- readRDS("Equaddata/cmsTest_3.rds")

### !!! time consuming code - only run when chaging code !!! ###
### bandwidth calculations ###
# bw.all_1 <- npregbw(formula = dld_1_formula, regtype = "ll", bwmethod = "cv.aic", bwtype="fixed", ckertype= "gaussian", data = Dta_sub_log)
# bw.all_2 <- npregbw(formula = dld_2_formula, regtype = "ll", bwmethod = "cv.aic", bwtype="fixed", ckertype= "gaussian", data = Dta_sub_log)
# bw.all_3 <- npregbw(formula = dld_3_formula, regtype = "ll", bwmethod = "cv.aic", bwtype="fixed", ckertype= "gaussian", data = Dta_sub_log)
# 
# saveRDS(bw.all_1, file= "Equaddata/bw.all_1.rds")
# saveRDS(bw.all_2, file= "Equaddata/bw.all_2.rds")
# saveRDS(bw.all_3, file= "Equaddata/bw.all_3.rds")
# saveRDS(bw.all_1, file= "Equaddata/bw.all_1.1.rds")
# saveRDS(bw.all_2, file= "Equaddata/bw.all_2.1.rds")
# saveRDS(bw.all_3, file= "Equaddata/bw.all_3.1.rds")

# bw.all_1 <- readRDS("Equaddata/bw.all_1.rds")
# bw.all_2 <- readRDS("Equaddata/bw.all_2.rds")
# bw.all_3 <- readRDS("Equaddata/bw.all_3.rds")
bw.all_1 <- readRDS("Equaddata/bw.all_1.1.rds")
bw.all_2 <- readRDS("Equaddata/bw.all_2.1.rds")
bw.all_3 <- readRDS("Equaddata/bw.all_3.1.rds")

# summary(bw.all_1)
# summary(bw.all_2)
# summary(bw.all_3)

model_1.np <- npreg(bws = bw.all_1, gradients = TRUE, residuals = TRUE)
model_2.np <- npreg(bws = bw.all_2, gradients = TRUE, residuals = TRUE)
model_3.np <- npreg(bws = bw.all_3, gradients = TRUE, residuals = TRUE)

# summary(model_1.np)
# summary(model_2.np)
# summary(model_3.np)

### !!! time consuming code - only run when chaging code !!! ###
#Kernel Regression Significance Test
# KRS_test_1 <- npsigtest(model_1.np)
# KRS_test_2 <- npsigtest(model_2.np)
# KRS_test_3 <- npsigtest(model_3.np)
# 
# saveRDS(KRS_test_1, file= "Equaddata/KRS_test_1.rds")
# saveRDS(KRS_test_2, file= "Equaddata/KRS_test_2.rds")
# saveRDS(KRS_test_3, file= "Equaddata/KRS_test_3.rds")
# saveRDS(KRS_test_1, file= "Equaddata/KRS_test_1.1.rds")
# saveRDS(KRS_test_2, file= "Equaddata/KRS_test_2.1.rds")
# saveRDS(KRS_test_3, file= "Equaddata/KRS_test_3.1.rds")

# KRS_test_1 <- readRDS("Equaddata/KRS_test_1.rds")
# KRS_test_2 <- readRDS("Equaddata/KRS_test_2.rds")
# KRS_test_3 <- readRDS("Equaddata/KRS_test_3.rds")
KRS_test_1.1 <- readRDS("Equaddata/KRS_test_1.1.rds")
KRS_test_2.1 <- readRDS("Equaddata/KRS_test_2.1.rds")
KRS_test_3.1 <- readRDS("Equaddata/KRS_test_3.1.rds")

e1x_np <- list()
e1x_np$e11_np <- mean(model_1.np$grad[,1])
e1x_np$e12_np <- mean(model_1.np$grad[,2])
e1x_np$e13_np <- mean(model_1.np$grad[,3])
e1x_np$eta1_np <- mean(model_1.np$grad[,4])

e1x_np_t <- list()
e1x_np_t$e11_np_t <- mean(model_1.np$grad[,1])/mean(model_1.np$gerr[,1])
e1x_np_t$e12_np_t <- mean(model_1.np$grad[,2])/mean(model_1.np$gerr[,2])
e1x_np_t$e13_np_t <- mean(model_1.np$grad[,3])/mean(model_1.np$gerr[,3])
e1x_np_t$eta1_np_t <- mean(model_1.np$grad[,4])/mean(model_1.np$gerr[,4])

e2x_np <- list()
e2x_np$e21_np <- mean(model_2.np$grad[,1])
e2x_np$e22_np <- mean(model_2.np$grad[,2])
e2x_np$e23_np <- mean(model_2.np$grad[,3])
e2x_np$eta2_np <- mean(model_2.np$grad[,4])

e2x_np_t <- list()
e2x_np_t$e21_np_t <- mean(model_2.np$grad[,1])/mean(model_2.np$gerr[,1])
e2x_np_t$e22_np_t <- mean(model_2.np$grad[,2])/mean(model_2.np$gerr[,2])
e2x_np_t$e23_np_t <- mean(model_2.np$grad[,3])/mean(model_2.np$gerr[,3])
e2x_np_t$eta2_np_t <- mean(model_2.np$grad[,4])/mean(model_2.np$gerr[,4])

e3x_np <- list()
e3x_np$e31_np <- mean(model_3.np$grad[,1])
e3x_np$e32_np <- mean(model_3.np$grad[,2])
e3x_np$e33_np <- mean(model_3.np$grad[,3])
e3x_np$eta3_np <- mean(model_3.np$grad[,4])

# par(mfrow=c(1,3))
# plot(density(model_1.np$grad[,1]), sub = "Meats - own price elasticity", main= "", xlim=c(-5,5))
# plot(density(model_2.np$grad[,2]), sub = "Dairy - own price elasticity", main= "",xlim=c(-5,5))
# plot(density(model_3.np$grad[,3]), sub = "Beans - own price elasticity", main= "", xlim=c(-5,5))
# 
# plot(density(model_1.np$grad[,4]), sub = "Meats - income elasticity", main= "", xlim=c(-5,5))
# plot(density(model_2.np$grad[,4]), sub = "Dairy - income elasticity", main= "",xlim=c(-5,5))
# plot(density(model_3.np$grad[,4]), sub = "Beans - income elasticity", main= "", xlim=c(-5,5))

e3x_np_t <- list()
e3x_np_t$e31_np_t <- mean(model_3.np$grad[,1])/mean(model_3.np$gerr[,1])
e3x_np_t$e32_np_t <- mean(model_3.np$grad[,2])/mean(model_3.np$gerr[,2])
e3x_np_t$e33_np_t <- mean(model_3.np$grad[,3])/mean(model_3.np$gerr[,3])
e3x_np_t$eta3_np_t <- mean(model_3.np$grad[,4])/mean(model_3.np$gerr[,4])

exx_np <- matrix(unlist(c(e1x_np, e2x_np, e3x_np)), nrow = 3, byrow = TRUE)
exx_np_r2 <- matrix(c(signif(model_1.np$R2, 5), signif(model_2.np$R2, 5), signif(model_3.np$R2, 5)), ncol=1)

exx_np_t <- matrix(unlist(c(e1x_np_t, e2x_np_t, e3x_np_t)), nrow = 3, byrow = TRUE)

##############################################################################
### DLD_np - calculate signif on Marshallian demand elasticities matrix    ###
##############################################################################
i <- 1
j <- 1
temp <- matrix(nrow=3, ncol = 4, 
  dimnames = list(c("meats_lnq1", "dairy_lnq2", "beans_lnq3"), c("meats", "dairy", "beans", "income_elasticity"))
  )

while(i != nrow(exx_np_t)+1 && j != ncol(exx_np_t) ){
  
  for(j in 1:ncol(exx_np_t)){
    temp[i,j] <- paste(signif(exx_np[i,j], 4), calculate_pValue(exx_np_t[i,j], 156)[2], sep="")
    j <- j+1
  }
  i <- i+1
  j <- 1
}
exx_np_t_sig <- temp # put results into matrix

exx_np_wr2 <- cbind(exx_np_t_sig, exx_np_r2)
dimnames(exx_np_wr2) <- list(c("meats_lnq1", "dairy_lnq2", "beans_lnq3"), c("meats", "dairy", "beans", "income_elasticity", "R-squared"))


#########################################
### Full AIDS Model empirical results ###
#########################################
priceNames <- c("p1", "p2", "p3")
shareNames <- c("w1", "w2", "w3")


AIDS_estResult_constraint <- aidsEst( priceNames, shareNames, "X",
                       data = Dta_sub,
                       method = "IL", ILmaxiter = 1000, ILtol = 1e-10,
                       priceIndex = "S", hom = TRUE, sym = TRUE)

AIDS_estResult_no_constraint <- aidsEst( priceNames, shareNames, "X",
                       data = Dta_sub,
                       method = "IL", ILmaxiter = 1000, ILtol = 1e-10,
                       priceIndex = "S", hom = FALSE, sym = FALSE)

#summary(AIDS_estResult_constraint)
#summary(AIDS_estResult_no_constraint)

###########################################
### Full AIDS elasticities and t-values ###
###########################################
AIDS_elas_M_H_income_constraint <- elas( AIDS_estResult_constraint, method = "AIDS" )
AIDS_elas_M_H_income_no_constraint <- elas( AIDS_estResult_no_constraint, method = "AIDS" )

AIDS_elas_M_constraint <- matrix(AIDS_elas_M_H_income_constraint$marshall, nrow = 3)
AIDS_elas_income_constraint <- matrix(AIDS_elas_M_H_income_constraint$exp, nrow = 3)
AIDS_elas_M_income_constraint <- cbind(AIDS_elas_M_constraint, AIDS_elas_income_constraint)

AIDS_elas_M_constraint_t <- matrix(AIDS_elas_M_H_income_constraint$marshallTval, nrow = 3)
AIDS_elas_income_constraint_t <- matrix(AIDS_elas_M_H_income_constraint$expTval, nrow = 3)
AIDS_elas_M_income_constraint_t <- cbind(AIDS_elas_M_constraint_t, AIDS_elas_income_constraint_t)

#################################################################################
### Full AIDS - calculate signif on Marshallian demand elasticities matrix    ###
#################################################################################
i <- 1
j <- 1
temp <- matrix(nrow=3, ncol = 4, 
  dimnames = list(c("meats_lnq1", "dairy_lnq2", "beans_lnq3"), c("meats", "dairy", "beans", "income_elasticity"))
  )

while(i != nrow(AIDS_elas_M_income_constraint_t)+1 && j != ncol(AIDS_elas_M_income_constraint_t) ){
  
  for(j in 1:ncol(AIDS_elas_M_income_constraint_t)){
    temp[i,j] <- paste(signif(AIDS_elas_M_income_constraint[i,j], 4), calculate_pValue(AIDS_elas_M_income_constraint_t[i,j], 156)[2], sep="")
    j <- j+1
  }
  i <- i+1
  j <- 1
}
AIDS_elas_M_income_constraint_t_sig <- temp # put results into matrix

AIDS_elas_M_income_constraint_r2 <- matrix(signif(AIDS_estResult_constraint$r2, 5), ncol=1, dimnames = list(c("meats_lnq1", "dairy_lnq2", "beans_lnq3"), c("R-squared")))

AIDS_elas_M_income_constraint_t_sig_wr2 <- cbind(AIDS_elas_M_income_constraint_t_sig, 
                                                 AIDS_elas_M_income_constraint_r2)




###     ###
# weights <- c( lnp1_catfish=mean(Dta_sub$lnp1_catfish), lnp2_crawfish=mean(Dta_sub$lnp2_crawfish), lnp3_clams=mean(Dta_sub$lnp3_clams), lnp4_shrimp=mean(Dta_sub$lnp4_shrimp), lnp5_tilapia=mean(Dta_sub$lnp5_tilapia), lnp6_salmon=mean(Dta_sub$lnp6_salmon))
# 
# weights <- weights/sum(weights)
# 
# est1_np <- npregHom(yName = "lnq1_catfish", xNames = c("lnp1_catfish", "lnp2_crawfish", "lnp3_clams", "lnp4_shrimp", "lnp5_tilapia", "lnp6_salmon", "lnX"), data=Dta_sub, homWeights = weights)
# 
# est2_np <- npregHom(yName = "lnq2_crawfish", xNames = c("lnp1_catfish", "lnp2_crawfish", "lnp3_clams", "lnp4_shrimp", "lnp5_tilapia", "lnp6_salmon", "lnX"), data=Dta_sub, homWeights = weights)
# 
# e1x_np <- list()
# e1x_np$e11 <- mean(est1_np$est$grad[,1])
# e1x_np$e12 <- mean(est1_np$est$grad[,2])
# e1x_np$e13 <- mean(est1_np$est$grad[,3])
# e1x_np$e14 <- mean(est1_np$est$grad[,4])
# e1x_np$e15 <- mean(est1_np$est$grad[,5])
# e1x_np$e16 <- mean(est1_np$est$grad[,6])
# 
# e2x_np <- list()
# e2x_np$e21 <- mean(est2_np$est$grad[,1])
# e2x_np$e22 <- mean(est2_np$est$grad[,2])
# e2x_np$e23 <- mean(est2_np$est$grad[,3])
# e2x_np$e24 <- mean(est2_np$est$grad[,4])
# e2x_np$e25 <- mean(est2_np$est$grad[,5])
# e2x_np$e26 <- mean(est2_np$est$grad[,6])
# 

```
  
\newpage
  
# Introduction
  
Empirical demand analysis has been dominated by the use of parametric functional form models since the appearance of the Working-Lesser Model in the 1940's [@Working:1943wb] [@Leser:1963vd]. A 2015 paper by Clements and Gao [@Clements:2015wj] provide a citation count of journal articles related to four popular parametric demand models (Linear Expenditure, Rotterdam, Translog and Almost Ideal Demand). The authors considered multiple time periods over the date range of 1974-2013. Their results clearly show an upward trajectory for all four parametric demand models with the Linear Expenditure and Almost Ideal Demand (AIDS) models having the highest citation counts, both in the multiple thousands of citations. This result suggests that parametric demand modeling is still very much a hotly researched area.  
  
While parametric form models have clearly dominated the research landscape there are other empirical techniques available to evaluate consumer expenditure data sets. The goal of empirical demand analysis is to answer basic questions about consumer behavior and in many cases, use these answers to design and implement policy (government) or improve some aspect of business design to increase market exposure/profits (private industry). Beyond these practical objectives Hal Varian [@Varian:1982wa] suggests that, given a consumers expenditure data set the demand analyst should ask four basic questions concerning the consumers behavior,  
  
> 
(1) Consistency? Is the observerd data consistent with the utility maximizing model?
(2) Structure? Is the observed data consistent with a utility function with some special structure?
(3) Recoverability? How can the underlying utility function be recovered?
(4) Extrapolation? How can we forecast behavior in other circumstances?
  
With these goals in mind economists are constantly refining their tools to improve their estimates of consumer behavior.  
  
Nonparametric economic analysis has been a growing field of study over the past several decades. Many new techniques have been developed within a theoretical framework. However, despite the rapid growth of theoretical results, nonparametric applied research has lagged considerably. This may be due in part to the advanced mathematical and statistical exposition presented in many nonparametric research papers. It is often difficult for economist, while well trained in advanced mathematical techniques, to fully grasp the significance and translate from purely theoretical results into applied research. Theoretical researchers working within the field of mathematics and statistics often fail to consider applied economic problems and applied economist are not exploring these newly developed theoretical techniques and refining the theory once they have touched real world data. In addition, until recently, nonparametric techniques were substantially more computationally intensive compared to their parametric counterparts. While the previous statement would seem to imply that nonparametric techniques have now become computationally less intensive, in reality it is our computer hardware and programming techniques which have improved to the point where nonparametric analysis has become a viable alternative, especially where parametric techniques fall short. These improvements in computer hardware have effectively opened an area of research which, until recently, had remained closed.  
  
Regression analysis is the workhorse technique employed in econometrics. However, in linear regression it is assumed the regressors enter the conditional mean in a linear fashion and each regressor is independent of the other which is often a violation of the data under study. Even when we use nonlinear regression techniques we often still assume we know the functional form for the data generating process (DGP).  
  
The single largest potential issue with using a parametric form model to evaluate consumer demand is the prior imposition of functional form on the demand model. If a demand system is well represented by a functional form and the econometric model is correctly specified with theoretical assumptions met then a parametric estimator is both consistent and efficient. In fact if the previous criteria are met parametric regression is superior in just about every way. However, these requirements are excessively difficult to satisfy when used in the wild to gain insight about real world data and thus the parametric approach can be seriously flawed and worst, can lead the researcher to faulty conclusions which can have serious repercussions if used to implement policy.  
  
Nonparametric modelling affords many advantages over their parametric counterparts. Primary among them is the nonparametric models ability to help us uncover a more accurate representation of the unknown function, conditioned on the actual data in hand. In section 2 I will explore the methods and models used to estimate a nonparametric regression. In nonparametric analysis a critical step in achieving a reliable kernel estimator is choosing the correct bandwidth estimator. Some time will be spend exploring several different bandwidth estimators which are prominent in the current literature. I will also explore the two most popular kernel regression methods in current use, Local-Constant Least Squares (LCLS) and Local-Linear Least Squares (LLLS). I will summarize the data and quickly review the parametric models being used for comparison purposes. Section 3 explores the results obtained from the nonparametric kernel regression and compares these results with the parametric form models used on the same data set. Section 4 concludes and details direction for additional studies.  

# Models, Methods, and Data  
  
In this paper we seek to explore the validity of using nonparametric techniques to uncover estimates of demand elasticities. To this end we use the Log-Log demand system developed by Working [-@Working:1943wb] and Lesser [-@Leser:1963vd]. The benefit of using this type of parametric form is the sheer simplicity of the model. This simplicity allows us to easily compare the effectiveness of our nonparametric econometric technique without getting bogged down in complicated estimation considerations or theory constraints. The only theory constraints available to the Log-Log demand model is homogeneity of degree zero in prices and income. However, even though this constraint is available to the Log-Log model we decide to forego all economic theory constraints to make comparability more consistent.  
  
## Log-Log Demand Model  
  
Equation 1 shows the Log-Log model specification for the Marshallian own and cross prices elasticities ($e_{ik}$) and the income elasticities ($e_i$). Since we are unable to impose cross equation constraints our study will focus on the own price and income elasticities of this model. While the Hicksian elasticities are of more interest for policy and welfare studies, we focus only on the Marshallian estimates so a direct estimate comparison can be made between the parametric and nonparametric elasticity.  
  
\begin{equation} \label{marshal_unrestricted}
\log ({q_i}) = {\alpha _i} + {e_i}\log (x) + \sum\limits_{k = 1}^n {{e_{ik}}\log ({p_k})} 
\end{equation}
  
It is well known that a correctly specified parametric model is preferred over nonparametric methods. However, we also know that correct model specification prior to study is a near impossible task. It is important to test the parametric model to assess if the functional form is consistent with the DGP. The test we utilize in this study was developed by Hasiao, Li and Racine and is a test of Consistent Model Specification (CMS-test) [@Hsiao:2007uk].
  
## Consistent Model Specification Test  
  
The CMS-test is a kernel based evaluation which tests for correct specification of the parametric form model. A short review of the details of the test are given below.  
  
The null hypothesis is given by,  
  
${H_0}:P[E({y_i}|{x_i}) = m({x_i},\beta )] = 1{\text{ for some }}\beta  \in \mathscr{B}$  
  
where,  
  
$m( \cdot , \cdot )$ is a known function with $\beta$ being a $p \times 1$ vector of unknown parameters,  
  
$\mathscr{B}$ is a compact subset in ${\mathbb{R}^p}$.  
  
The alternative hypothesis is given by,  
  
${H_1}:P[E({y_i}|{x_i}) = m({x_i},\beta )] < 1{\text{ for all }}\beta  \in \mathscr{B}$
  
The test statistic was originally developed by Fan and Li [-@Fan:1996wr]  and Zheng [-@JohnXuZheng:1996uo].  
  
The test statistic is given by,  
  
${I_n} = \frac{1}{{{n^2}}}\sum\limits_i^{} {\sum\limits_{j \ne i}^{} {{{\hat u}_i}} } {\hat u_j}{K_{\gamma {j_{ij}}}}$
  
where,  
  
${K_{\gamma {j_{ij}}}} = {W_{h,ij}}{L_{\lambda ,ij}}(\gamma  = (h,\lambda ))$,  
  
${\hat u_i} = {y_i} - m({x_i},\hat \beta )$ is the parametric null model's residual,  
  
$\beta$ is a $\sqrt(n)$-consistent estimator of $\beta$ under $H_0$.  
  
Hsiao et al. [-@Hsiao:2007uk] advocate for the use of cross-validation methods for selecting the kernel smoothing parameter vectors, which is the approach we took in this analysis. Under assumptions (A1)-(A3) of their paper we can obtain the cross-validation based (CV-based) test with the new test statistic ${\hat J_n}$:  
  
$n{({\hat h_1},...,{\hat h_q})^{1/2}}{\hat I_n} \to N(0,\Omega )$ in distribution under $H_0$,  
  
where,  
  
$\Omega  = 2E\left[ {{\sigma ^4}({x_i})f({x_i})} \right]\left[ {\int {{W^2}(v)dv} } \right]$  
  
A consistent estimator of $\Omega$ is given by,  
  
$\hat \Omega  = {n^{ - 2}}2({\hat h_1},...,{\hat h_q})\sum\limits_i^{} {\sum\limits_{j \ne i}^{} {\hat u_i^2} } \hat u_j^2W_{\hat h,ij}^2L_{\hat \lambda ,ij}^2$  
  
Which gives the CV-based test statistic as,  
  
${\hat J_n} = n{({\hat h_1},...,{\hat h_q})^{1/2}}{\hat I_n}{\hat \Omega ^{ - 1/2}} \to N(0,1)$ in distribution under $H_0$.  
  
According to the authors, it can be easily shown that the $\hat J_n$ test statistic diverges to $+ \infty$ if $H_0$ is false.  
  
## Bandwidth Selection models  
  
The importance of proper bandwidth selection in nonparametric analysis is reviewed here. However, for a more detailed discussion of the methods and theory used in this paper see the excellent work by the following authors: Cheng et al.[-@Cheng:1997vy], Hall et al.[-@Hall:2007vq], Hurvich et al.[-@Hurvich:1998wi], Li and Racine[-@li2007nonparametric], Racine and Li[-@Racine:2004tua], and Li and Racine[-@Li:2004tn].  
  
The most popular methods for bandwidth selection in applied research are data driven, and of the data driven methods Least-Squares Cross Validation (LSCV) and expected Kullback-Leibler cross-validation also know in the literature as Akaike Information Criterion Cross Validation (aic-CV), are the two used in this paper to evaluate the validity of using nonparametric techniques to uncover demand elasticities.  
  
### Least-Squares Cross Validation (LSCV)  
  
While there are many methods to determine an optimal bandwidth, the most commonly used in the current literature is the cross-validation method of LSCV which is defined as:  
  
\begin{equation}
LSCV(h) = {\sum\limits_{i = 1}^n {\left[ {{y_i} - {{\hat m}_{ - 1}} - ({{\text{x}}_i})} \right]} ^2}
\end{equation}
  
where,  
  
${\hat m_{ - 1}} - ({{\text{x}}_i})$ is the leave-one-out estimator defined by Li and Racine[-@Li:2004tn] as,  
  
\begin{equation}
{\hat m_{ - 1}} - ({{\text{x}}_i}) = \frac{{\sum\limits_{\scriptstyle j \ne i \hfill \atop 
  \scriptstyle j = 1 \hfill} ^n {{y_h}({{\text{x}}_i},{{\text{x}}_j})} }}{{\sum\limits_{\scriptstyle j \ne i \hfill \atop 
  \scriptstyle j = 1 \hfill} ^n {{k_h}({{\text{x}}_i},{{\text{x}}_j})} }}
\end{equation}  
  
An examination of equation 2 reveals that our algorithm will require $n^2$ calculations. Several authors have suggested a derivation of the LSCV that side-steps this issue; we have decided to use the classic formula as defined in the original paper by Li and Racine[-@Li:2004tn]. It should be noted however, as the data sample grows, the calculations become computationally burdensome.  
  
Gaining in popularity is a cross-validation bandwidth selection method developed by Hurvich et al.[-@Hurvich:1998wi] called Akaike Information Criterion Cross Validation (aic-CV). This method is similar to Kullback-Leibler information criterion method which is the foundation for density estimation using likelihood cross-validation. However, the true benefit of using the aic-CV method is the property of reduced variability the tenancy for this method to undersmooth.  
  
### Akaike Information Criterion Cross Validation (aic-CV)  
  
The aic-CV method is defined as:  
  
\begin{equation}
AI{C_c}(h) = \ln ({\hat \sigma ^2}) + \frac{{1 + p/n}}{{1 - (p + 2)/n}} = \ln ({\hat \sigma ^2}) + 1 + \frac{{2(p + 1)}}{{n - p - 2}}
\end{equation}
  
where,  
  
$AIC_C$ is the bias corrected version,  
  
$\hat \sigma^2$ is the estimated error variance,  
  
p is the number of regression (or autoregressive) parameters in the model.  
  
which gives the following formula for smoothing parameter selection:  
  
\begin{equation}
AI{C_c}(h) = \ln ({\hat \sigma ^2}) + \frac{{1 + tr(H)/n}}{{1 - \{ tr(H) + 2\} /n}} = \ln ({\hat \sigma ^2}) + 1 + \frac{{2(tr(H) + 1)}}{{n - tr(H) - 2}}
\end{equation}
  
where,  
  
${\hat \sigma ^2} = {n^{ - 1}}{\sum\limits_{i = 1}^n {\left[ {{y_i} - \hat m({{\text{x}}_i})} \right]} ^2}$,  
  
and H is called the hat matrix or smoother matrix defined by:  
  
\[H = \left[ {\begin{array}{*{20}{c}}
  {\frac{{K({{\text{x}}_1}{\text{,}}{{\text{x}}_1})}}{{\hat f({{\text{x}}_1})}}}&{\frac{{K({{\text{x}}_1}{\text{,}}{{\text{x}}_1})}}{{\hat f({{\text{x}}_1})}}}& \vdots &{\frac{{K({{\text{x}}_1}{\text{,}}{{\text{x}}_n})}}{{\hat f({{\text{x}}_1})}}} \\ 
  {\frac{{K({{\text{x}}_2}{\text{,}}{{\text{x}}_1})}}{{\hat f({{\text{x}}_2})}}}&{\frac{{K({{\text{x}}_2}{\text{,}}{{\text{x}}_2})}}{{\hat f({{\text{x}}_2})}}}& \vdots &{\frac{{K({{\text{x}}_2}{\text{,}}{{\text{x}}_n})}}{{\hat f({{\text{x}}_2})}}} \\ 
   \vdots & \vdots & \ddots & \vdots  \\ 
  {\frac{{K({{\text{x}}_n}{\text{,}}{{\text{x}}_1})}}{{\hat f({{\text{x}}_n})}}}&{\frac{{K({{\text{x}}_n}{\text{,}}{{\text{x}}_2})}}{{\hat f({{\text{x}}_n})}}}& \cdots &{\frac{{K({{\text{x}}_n}{\text{,}}{{\text{x}}_n})}}{{\hat f({{\text{x}}_n})}}} 
\end{array}} \right]\]
  
The trace of the H matrix should be thought of as a measure of the number of parameters in the nonparametric model [@Hastie:1993vz].  
  
## Nonparametric Regression models  
  
While the method of bandwidth selection is critical, the end game of this analysis is to use the bandwidth estimator in a regression analysis to evaluate a comparison between elasticities derived from a parametric model estimated under Least Squares versus elasticities derived from nonparametric regression methods. With this goal in mind we now explore nonparametric regression methods.  
  
### Local-Constant Least Squares (LCLS)  
  
The LCLS regression was the first nonparametric regression model developed and is sometimes referred to as the Nadaraya–Watson estimator after Nadaraya [-@Nadaraya:1964de] and Watson [-@Watson:1964vz]. This method minimizes the kernel weighted least-squares regression of $y_i$ on a constant $c$. If we assume some function $g$ s.t. $y_i=g(x_i)+\epsilon_i$ then our objective with LCLS is to approximate $g(\cdot)$ with a constant $c$. More formally the LCLS can be defined as:  
  
\begin{equation}
LCLS(y;{\text{x}}) = \frac{1}{{n{h_y}|{\text{h}}|}}\sum\limits_{i = 1}^n k \left( {\frac{{{y_i} - y}}{{{h_y}}}} \right){K_h}({{\text{x}}_i}{\text{,x}})
\end{equation}
  
where,  
  
${K_h}({{\text{x}}_i}{\text{,x}})$ is the product kernel.

\begin{equation}
a = \arg \min \left\{ {\hat m({\text{x}})} \right\} = \frac{{\sum\limits_{i = 1}^n {{K_h}({{\text{x}}_i}{\text{,x}}){y_i}} }}{{\sum\limits_{i = 1}^n {{K_h}({{\text{x}}_i}{\text{,x}})} }}
\end{equation}
  
where $a$ can be thought of as our constant $c$ which approximates $g(\cdot)$.  
  
The above estimator can be represented in matrix form as:  
  
$$\hat m({\text{x}}) = {(\textbf{i}'K({\text{x}})\textbf{i})^{ - 1}}\textbf{i}'K({\text{x}})y$$
  
where,  
  
$\textbf{i}$ is the identity matrix of dimension $n \times 1$,  
  
and $K({\text{x}})$ is the same product kernel from equation 6,  
  
$$K({\text{x}}) = \left[ {\begin{array}{*{20}{c}}
  {{K_h}({{\text{x}}_1}{\text{,x}})}&0&0&0 \\ 
  0&{{K_h}({{\text{x}}_2}{\text{,x}})}&0&0 \\ 
  0&0& \ddots &0 \\ 
  0&0&0&{{K_h}({{\text{x}}_n}{\text{,x}})} 
\end{array}} \right]$$
  
One of the problems with the LCLS estimator is that it is bias. This may not be of concern depending on the circumstances but it should be noted.



### Local-Linear Least Squares (LLLS)  
  
The LLLS estimator has been growing in popularity as an alternative to the LCLS method. In addition to increasing use in applied research the LLLS estimator has been dominated the theoretical literature.  
  
Conceptually, the LLLS estimator is similar to the LCLS with the exception that instead of fitting a constant to our left hand side variable we now consider fitting a locally defined line segment. This allows us to more accurately model the data and is especially useful when we have outliers and nonlinear relationships in our data.  
  
We can think of our local-linear fitting as a local Taylor Series expansion at some point \textbf{x}. In other words, for the relationship $y=m(x)+u$, we have paired data on $(y_1,x_1), (y_2,x_2), ..., (y_n,x_n)$. This allows us to take a local Taylor approximation for each of the points \textbf{x}. It now become clear, with locally fitting with kernel weighting, problematic data (outliers, nonlinearity) can be better modeled by LLLS.  
  
The LLLS estimator as developed and defined by Li and Racine [-@Li:2004tn] is given as follows:  
  
We begin with a Taylor expansion of \textbf{x} for every observation $i$ as,  
  
$${y_i} = m({x_i}) + {u_i} \approx m(x) + ({x_i} - x)\beta (x) + {u_i}$$  
  
where,  
  
$(x_i-x)$ is a $1 \times q$ vector and $\beta(x)$ is the gradient vector of dimension $q$.  
  
then,  
  
$${y_i} = \alpha  + ({x_i} - x)b + {u_i} = \left[ {\begin{array}{*{20}{c}}
  1&{({x_i} - x)} 
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
  a \\ 
  b 
\end{array}} \right] + {u_i}$$  
  
where,  
  
$m(x)$ and $\beta(x)$ are parameters.  
  
Our minimization problem then becomes,  
  
$$\arg \mathop {\min }\limits_{a,b} {\sum\limits_{i = 1}^n {\left[ {{y_i} - a - ({x_i} - x)b} \right]} ^2}{K_h}({x_i},x)$$  
and in matrix notation,  
  
$$\arg \mathop {\min }\limits_\delta  {\left[ {y - X\delta } \right]^\prime }K(x)\left[ {y - X\delta } \right]$$  
  
where,  
  
$\delta=(a,b)^{\prime}$,  
  
$X$ is a $n \times (q+1)$ matrix with the $i^{th}$ row equal $\left[ {\begin{array}{*{20}{c}}
  1&{({x_i} - x)} 
\end{array}} \right]$,  
  
$K(x)$ is $n \times n$ diagonal matrix with the $i^{th}$ element equal to $K_h(x_i,x)$.  
  
If $K(x)$ is an identity matrix then we simply have the OLS estimator.  
  
If $K(x)$ is equal to $\Omega=vcov(\theta)$ then the above relationship reduces to the generalized least squares.  
  
When we minimize the objective function $\hat \delta(x)$ we get:  
  
\begin{equation*}
\begin{split}
\hat \delta (x) & = \left[ {\begin{array}{*{20}{c}}
  {\hat m(x)} \\ 
  {\hat \beta (x)} 
\end{array}} \right] \\ 
  &  = {\left[ {\sum\limits_{i = 1}^n {{K_h}({x_i},x)\left( {\begin{array}{*{20}{c}}
  1 \\ 
  {{x_i} - x} 
\end{array}} \right)\left( {\begin{array}{*{20}{c}}
  1&{({x_i} - x)} 
\end{array}} \right)} } \right]^{ - 1}}\sum\limits_{i = 1}^n {{K_h}({x_i},x)\left( {\begin{array}{*{20}{c}}
  1 \\ 
  {{x_i} - x} 
\end{array}} \right){y_i}}  \\
&  = {\left[ {{X^\prime }K(x)X} \right]^{ - 1}}{X^\prime }K(x)y \\
\end{split}
\end{equation*}
  
where the final equation in matrix form gives our LLLS estimators.  
  
The bias of the LLLS estimator is less complicated to work with than the LCLS and deals better with nonlinear and irregular data. For these reasons many researchers employ the LLLS estimator over the LCLS. However, there are situations where the LCLS estimator will be preferred so an analysis of both methods within the context of the problem is always wise.  
  
With parametric regression analysis it is customary to evaluate the overall significance of the regression. In nonparametric regression we employ a analogous test referred to as Kernel Regression Significance Test as developed by Racine et al. [-@Racine:2006wf] and Racine [-@Racine:1997vd].  
  
##Kernel Regression Significance Test
  
The Kernel Regression Significance Test developed by Racine et al. [-@Racine:2006wf] and Racine [-@Racine:1997vd] is applicable to mixed nonparametric regressions, that is, the test will work with a combination of discrete and continuous regressors.  
  
The null hypothesis and the test statistic are as follows:  
  
${H_0}:E(y|x,z) = E(y|x)$ almost everywhere (a.e.)
  
where,  
  
$z$ denotes categorical or continuous variables,  
  
$x$ denotes the remaining explanatory variables,  
  
$y$ denotes the dependent variable.
  
Then the test statistic given by Racine et al. [-@Racine:2006wf]:  
  
$${\hat I_n} = \frac{1}{n}\sum\limits_{i = 1}^n {\sum\limits_{l = 1}^{c - 1} {{{\left[ {\hat m({x_i},{z_i} = l) - \hat m({x_i}{z_i} = 0)} \right]}^2}} }$$
  
where,  
  
$\hat m({x_i}{z_i} = l) = \frac{{\sum\limits_{j = 1}^n {{y_i}{W_{\hat h,ij}}{L_{\hat \lambda ,ij}}{l_{zj,z = l,\hat \lambda z}}} }}{{\sum\limits_{j = 1}^2 {{W_{\hat h,ij}}{L_{\hat \lambda ,ij}}{l_{zj,z = l,\hat \lambda z}}} }}$  
  
$\hat I_n \to 0$ in probability under $H_0$.  
  
## Methods  
  
The goal of this study is to investigate the effectiveness of using nonparametric regression analysis to recover demand elasticities for a group of three commodities; meats, dairy, and beans. To facilitate the comparison we will estimate equation by equation Ordinary Least Squares regression on the Log-Log demand model (equation 1) developed by Working and Lesser. The reason for choosing this model is the sheer simplicity of the functional form which allows us to easily estimate the model using OLS and nonparametric regression techniques without the concern of complicated estimation procedures or elaborate constraint requirements. This simplicity will allow us to compare the two models as closely as possible to gauge how effect the nonparametric technique is at recovering the elasticities.  
  
Having assumed the Log-Log demand model functional form we use the Consistent Model Specification Test detailed in section 2.2 to ascertain how well the Log-Log functional form does at recovering the true DGP of our date set. An improper specification of functional form will lead to incorrect parameter estimates. The test is run three times, once for each commodity according to equation 1. In all three cases we preform the test by the following configuration: The distribution was estimated using iid bootstrapping methods with 1000 replications. A pivot table was used so that the distribution approached $N(0,1)$. A random seed was used in the analysis to enable reproducibility and is embedded within the code (available upon request).  
  
The Log-Log demand model is estimated equation by equation using standard OLS regression. The only theory constraint available to the model is homogeneity, however, no constraints were imposed on the data in this study. There was no correction for heterogeneity and all variables are assumed to be exogenous, a faulty assumption in the current context, but an assumption that is ignorable given the goal of this study.  
  
The nonparametric regression requires the selection of a bandwidth estimation technique. In this study we consider two methods of bandwidth estimation, Least-Squares Cross Validation (LSCV), and Akaike Information Criterion Cross Validation (aic-CV). Within both of these bandwidth estimators there are numerous variations available to fine tune the optimal bandwidth selection. Aside from the difference of base technique, ie. LSCV and aic-CV the configuration is identical between the two methods and follows as given: We use a Fixed bandwidth selection method. We used a Gaussian kernel for all studies, although the Epanechnikov kernel was evaluated and rejected for lack of performance. The kernel order was 2 in all studies. We did not employ any bandwidth scaling in any of these studies.  
  
The nonparametric regression techniques used in this study are the Local-Constant Least Squares and the Local-Linear Least Squares. With both of these techniques we estimated the residuals of the regression and the gradients, which are of interest when comparing the elasticities obtained by the parametric regression versus the nonparametric regression.  
  
A test of the overall regression significance of the nonparametric regression is preformed using the Kernel Regression Significance Test detailed in section 2.5. The test follows the algorithms detailed in Racine et al. [-@Racine:2006wf] with the specifics as follows: iid bootstrapping with 1000 replications was preformed. Bootstrap method type 1 was utilized. A pivitol statistic was calculated by dividing the calculated gradients by their respective standard errors. A random seed was embedded within the code to ensure reproducibility.

## Data  
  
The data were obtained from the 2006 Ecuadorian consumer expenditure survey and consist of three commodities, meats, dairy, and beans. For all three commodities we have price and quantity data for 2066 observations. For the Log-Log model the natural logarithm for quantities and prices was taken and passed to the model. For consistency the nonparametric regression uses the same logarithmic values. See table 1 (below) for summary statistics.  
  
```{r, echo=FALSE, message=FALSE, results='asis', cache=T}

stargazer(Dta_sub, header=F, type="latex", summary = TRUE, font.size = "small", notes= c("good 1 = meats", "good 2 = dairy", "good 3 = beans"), notes.align= "l", flip = F, float = T, float.env = "table", title="Summary Statistics")

```
  
# Results and Discussion  
  
The primary focus of this paper is to access the validity of using nonparametric regression methods to recover demand elasticities from a consumer consumption data set. In traditional demand analysis a parametric function form model is often picked for its simplicity of estimation or tractability or both. However, the validity of the parameters estimated form a misspecified demand model are called into question when the functional form is assumed and assumed incorrectly. Testing methods have been developed to ascertain the validity of using a parametric form to uncover the underlying DGP. The test we employed in this analysis is the Consistent Model Specification Test. In all three cases the Null of correct specification is rejected at the 0.1% level or greater. This indicates that the Log-Log specification does a poor job at recovering the true DGP for our data set.  
  
```{r, echo=FALSE, message=FALSE, results='asis', cache=T}

stargazer(exx_par_wr2, header=F, type="latex", summary = FALSE, font.size = "small", notes= c("***Significant at the 1 percent level,", "**Significant at the 5 percent level,", "*Significant at the 10 percent level."), notes.align= "r", flip = F, float = T, float.env = "table", title="Parametric - Double Log Demand Model")

stargazer(exx_np_wr2, header=F, type="latex", summary = FALSE, font.size = "small", notes= c("***Significant at the 1 percent level,", "**Significant at the 5 percent level,", "*Significant at the 10 percent level."), notes.align= "r", flip = F, float = T, float.env = "table", title="Nonparametric Regression using Gaussian Kernel")

```
  
Table 2 shows the results from the parametric OLS regression of the Log-Log demand model. When we examine the R-squared values for the three equations we notice that equation 1 (meats) does have a relatively high R-squared value which seems to indicate a reasonable fit for this model. However, when we move to evaluate equation 2 (dairy) and equation 3 (beans) we notice progressively decreasing R-squared values which indicates, especially for equation 3, that our OLS regression does not fit the data well enough to be confident that our parameter estimates are reliable.  
  
Table 3 shows the results from the nonparametric regression. When we evaluate the R-squared values of the nonparametric regression we notice a  systematic increase in value. All three nonparametric regressions give higher R-squared values over the parametric regression models. This indicates an improvement in model fit using nonparametric techniques.  
  
Since the parametric form model we have chosen to work with only permits within equation constraints it only makes sense to make comparisons of own price and income elasticities. When we compare the own price elasticities of the parametric regression to the elasticities obtained from the nonparametric regression we notice the nonparametric own price elasticities are all greater in magnitude. When we look at the income elasticities between the two regressions the nonparametric model has lower values. However, the numeric values are all very close and the signs are all in agreement for the elasticities that are significant.  
  
  Due to the mechanism of estimation of the nonparametric regression a one for one parameter estimate is complicated by the fact that nonparametric regressions make a full range of estimates as opposed to parametric regressions which make point estimates of elasticities. This means we can get a better sense of how the elasticities are distributed across the data by plotting the gradients, in our case the actual elasticity estimates, and viewing the distribution of the elasticities across the entire sample range.  
  
The plots below show the distribution for the own price and income elasticities. It becomes clear that we have relatively tight clustering of own price elasticities for meats and dairy but we an unusual distribution of own price elasticity on beans. We have a primary peak with two smaller peaks of elasticities to either side of the primary peak. With the income elasticities we see relatively tight clustering for all three commodities.  
  
```{r, echo=FALSE, message=FALSE, results='asis', fig.height=3, cache=T}

par(mfrow=c(1,3))
plot(density(model_1.np$grad[,1]), sub = "Meats - own price elasticity", main= "", xlim=c(-5,5))
plot(density(model_2.np$grad[,2]), sub = "Dairy - own price elasticity", main= "",xlim=c(-5,5))
plot(density(model_3.np$grad[,3]), sub = "Beans - own price elasticity", main= "", xlim=c(-5,5))

plot(density(model_1.np$grad[,4]), sub = "Meats - income elasticity", main= "", xlim=c(-5,5))
plot(density(model_2.np$grad[,4]), sub = "Dairy - income elasticity", main= "",xlim=c(-5,5))
plot(density(model_3.np$grad[,4]), sub = "Beans - income elasticity", main= "", xlim=c(-5,5))

```
  
The Almost Ideal Demand System (AIDS) developed by Deaton and Muellbauer [-@Deaton:1980vf], [@Deaton:1980vh] has been one of the most highly used demand system specifications since its introduction in the 1980's. Monte Carlo studies have shown the AIDS specification to be effective at recovering the "true"" elasticities of demand under a variety of situations [@Barnett:2008ut]. For this reason, we will use the AIDS model as a reality check against our Log-Log demand model and our nonparametric regression model. We will compare the elasticities estimated between all three model specifications to gain insight about how effective our nonparametric regression is at recovering demand elasticities.  
  
The Full AIDS model takes the following functional form:  
  
\begin{equation*}
{w_i} = {\alpha _i} + \sum\limits_{j = 1}^n {\gamma _{ij}^{}\ln {p_j} + {\beta _i}\ln \left( {\frac{X}{{{P}}}} \right)}  + {\mu _i}
\end{equation*}
  
where,   
  
${lnP} =$  a price index $= {\alpha _0} + \sum\limits_j^{} {{\alpha _j}{p_j}}  + \frac{1}{2}\sum\limits_j^{} {\sum\limits_i^{} {{\gamma _{ij}}\ln {p_i}\ln {p_j}} }$  
  
It should be readily apparent that the Full AIDS model can not be estimated using strictly linear estimation techniques. The non-linear AIDS model has been estimated by full information maximum likelihood (FIML) and by non-linear SUR methods, however, these methods suffer from difficulties with convergence. Several authors have shown that an iterated linear estimator can be consistently and asymptotically efficiently estimated by the Iterated Linear Least Squares Estimator (ILLE) developed by Blundell and Robin [-@Blundell:1999tw]. It is this iterated estimation method that is employed in this analysis of the Full AIDS model.  
  
Although the ILLE method is computationally efficient, the algorithm converges more quickly if starting values are passed on the first iteration. In this study, the initial parameter values were obtained by first estimating the LA/AIDS model using Stone's price index, then passing the estimated parameter values to the ILLE algorithm as the initial starting values. Economic theory restrictions were imposed on the AIDS model, homogeneity and symmetry.  
  
The Marshallian own and cross price elasticities are given by the following formula:  
  
\begin{equation}
{\varepsilon _{ij}} =  - {\delta _{ij}} + \frac{{{\gamma _{ij}}}}{{{w_i}}} - {\beta _i}\frac{{{w_j}}}{{{w_i}}} - \frac{{{\beta _i}}}{{{w_i}}}\sum\limits_k^{} {{\gamma _{kj}}\ln {p_k}}
\end{equation}
  
The income elasticity is:  
  
\begin{equation}
{\eta _i} = 1 + \frac{{{\beta _i}}}{{{w_i}}}
\end{equation}
  
The standard errors of the elasticities are estimated using the Delta Method.  
  
Table 4 shows the Marshallian elasticities obtained from the AIDS model. When we compare these results to our parametric regression and nonparametric regression we see the nonparametric version obtains elasticities closer in values to the AIDS model estimates. We see this trend in both the own price and income elasticities of the nonparametric model and the AIDS model. This result seems to confirm the nonparametric regression does a better job at recovering the demand elasticities in this specific case.  
  
```{r, echo=FALSE, message=FALSE, results='asis', cache=T}

stargazer(AIDS_elas_M_income_constraint_t_sig_wr2, header=F, type="latex", summary = FALSE, font.size = "small", notes= c("***Significant at the 1 percent level,", "**Significant at the 5 percent level,", "*Significant at the 10 percent level."), notes.align= "r", flip = F, float = T, float.env = "table", title="Full AIDS - Marshallian")

```
  
# Conclusions
  
We have explored the feasibility of using nonparametric regression analysis to recover the own price and income elasticities from a consumer expenditure data set. Using the nonparametric techniques we were able to obtain own price and cross price elasticities that were in agreements with those obtained using the Log-Log demand model, a parametric form model. We confirmed these results by estimating the non-linear AIDS model which also agreed with the elasticities we obtained by using nonparametric regression.  
  
When parametric model specification is taken as known and this assumption is correct, the parametric form regression produces both consistent and efficient estimates. However, we can run into problems when the assumed model does not represent the underlying data generating process. As a result, the parameter estimates obtained by the model will be faulty and could lead to wrong conclusions which has the potential to negatively effect policy.  
  
There are trade-offs when employing  nonparametric techniques. When we estimate parameters using unconstrained nonparametric regression we weaken the conclusions we can draw about commodity substitution and complementarity. In fact, this weakening of economic conclusions can be a pervasive problem when estimating using unconstrained nonparametric techniques. Work has begun to allow the incorporation of economic theory constraints into nonparametric methods, if so desired. An additional issue with using nonparametric methods is the much larger computational requirements. This issue is diminishing as computer hardware and efficient algorithm development move forward but a well specified parametric model requires a small fraction of the computing resource as a comparably specified nonparametric model.  
  
Despite these trade-offs, nonparametric methods do bring much to the table. nonparametric techniques can do a good job at uncovering functional forms which can potentially be used to develop more accurate parametric form models. nonparametric methods are considered local estimators as opposed to the parametric models which are global in nature. This local estimator gives us better information at specific points across the entire data range. We realized this advantage when evaluating the gradient (elasticity) plots in section 3. The unusual elasticity distribution on the own price on beans would have been obscured using parametric techniques. This type of data evaluation is only available when using nonparametric regression analysis.  
  
We have successfully showed how nonparametric regression can be used to uncover demand elasticities for a group of commodities within a consumer expenditure data set. This method could be extended by evaluating a more complex set of data. We would also be interested to preform Monte Carlo studies as an additional comparison point. With the emergence of nonparametric Seemingly Unrelated Regression and constrained nonparametric methods we would be interested in evaluating the effectiveness of nonparametric regression against a more informative demand model such as the ADIS (and variants of) and the Exact Affine Stone Index (EASI). We feel that nonparametric techniques have been underutilized in demand analysis and could help propel the field forward with providing useful insights which have been difficult to uncover.

\newpage

# References  
