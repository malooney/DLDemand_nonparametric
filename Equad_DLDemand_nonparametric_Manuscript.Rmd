---
title: "Nonparametric Demand Analysis"
author: "Matthew Aaron Looney"
date: "08/22/2017"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: no
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '4'
header-includes:
- \usepackage{graphicx}
- \usepackage{rotating}
- \usepackage{longtable}
- \usepackage{amssymb,amsmath}
- \usepackage{dcolumn}
- \usepackage{setspace}\doublespacing

abstract: "Nonparametric econometric analysis has been a growing field of study over the past several decades. Many new techniques have been developed within a theoretical framework. However, despite the rapid growth of theoretical results, nonparametric applied research has lagged considerably. This paper employs a nonparametric regression analysis within the context of demand theory. Data on prices and quantites of three commodities (meats, dairy products and beans) from the 2006 Ecuadorian consumer expenditure survey will be evaluated to derive Marshallian price and income elasticities. The nonparametric results will be compared with standard parametric demand analysis tools such as the Log-Log demand model and the Almost Ideal Demand system to guage the effectivness of using nonparametric techniques to estimate demand elasticities."

bibliography: np_demand_paper.bib
fontsize: 12pt
geometry: margin = 1.2in
---

```{r House Keeping, echo=FALSE, message=FALSE, cache=TRUE}

rm(list=ls())

# randNum <- round(runif(1, 1, 1e8)) # !!! RUN ONLY ONCE TO GENERATE RN for seed !!!
# randNum
set.seed(38532842) # all sequential simulations will start from this seed

library(readxl)
library(stargazer)
library(systemfit)
library(np)
library(micEconAids)

options(np.messages=FALSE) 
#options(digits=5)

setwd("~/Google Drive/digitalLibrary/*AAEC 6310 Demand and Price Analysis/DLDemand_nonparametric")

source("~/Google Drive/digitalLibrary/*AAEC 6310 Demand and Price Analysis/demandLab1/calculate_pValue.R")
source("~/Google Drive/digitalLibrary/*AAEC 6310 Demand and Price Analysis/demandLab1/calculate_tValues.R")

```

```{r Import & data manip, echo=FALSE, message=FALSE, cache=TRUE}

Dta <- read.csv("/Users/malooney/Google Drive/digitalLibrary/*Econometrics2/Lab3/protein.csv")
Dta_sub <- data.frame(obs=1:nrow(Dta))

Dta_sub$p1 <- Dta$pmeats1
Dta_sub$p2 <- Dta$pdairy1
Dta_sub$p3 <- Dta$ppulses1

Dta_sub$q1 <- Dta$qmeats
Dta_sub$q2 <- Dta$qdairy
Dta_sub$q3 <- Dta$qpulses

Dta_sub$p1q1 <- Dta$qmeats*Dta$pmeats1
Dta_sub$p2q2 <- Dta$qdairy*Dta$pdairy1
Dta_sub$p3q3 <- Dta$ppulses1*Dta$qpulses

Dta_sub$X <- Dta_sub$p1q1 + Dta_sub$p2q2 + Dta_sub$p3q3

Dta_sub$w1 <- Dta_sub$p1q1 / Dta_sub$X
Dta_sub$w2 <- Dta_sub$p2q2 / Dta_sub$X
Dta_sub$w3 <- Dta_sub$p3q3 / Dta_sub$X

Dta_sub_log <- Dta_sub
Dta_sub_log <- log(Dta_sub[,c(2:7,11)])

```

```{r Single Equation Model Estimation using Log-Log Functional Form, echo=FALSE, message=FALSE, cache=TRUE}

# Single Equation Model Estimation using Log-Log Functional Form
# Estimate Marshallian elasticities - unrestricted

dld_marshallian_1.lm <- lm(q1 ~ p1+ p2+ p3+ X, data=Dta_sub_log)
dld_marshallian_2.lm <- lm(q2 ~ p1+ p2+ p3+ X, data=Dta_sub_log)
dld_marshallian_3.lm <- lm(q3 ~ p1+ p2+ p3+ X, data=Dta_sub_log)

smry_dld_marshallian_1 <- summary(dld_marshallian_1.lm)
smry_dld_marshallian_2 <- summary(dld_marshallian_2.lm)
smry_dld_marshallian_3 <- summary(dld_marshallian_3.lm)

exx_par <- matrix(unlist(c(coef(dld_marshallian_1.lm)[2:5], coef(dld_marshallian_2.lm)[2:5], coef(dld_marshallian_3.lm)[2:5])), nrow = 3, byrow = TRUE)

exx_par_t <- matrix(
  unlist(c( 
  coef(dld_marshallian_1.lm)[2:5]/sqrt(diag(vcov(dld_marshallian_1.lm)[-1,-1])), 
  coef(dld_marshallian_2.lm)[2:5]/sqrt(diag(vcov(dld_marshallian_2.lm)[-1,-1])), 
  coef(dld_marshallian_3.lm)[2:5]/sqrt(diag(vcov(dld_marshallian_3.lm)[-1,-1]))
  )), 
  nrow = 3, byrow = TRUE)

######################################################################################
### DLD_parametric - calculate signif on Marshallian demand elasticities matrix    ###
######################################################################################
i <- 1
j <- 1
temp <- matrix(nrow=3, ncol = 4, 
  dimnames = list(c("meats_lnq1", "dairy_lnq2", "pulses_lnq3"), c("meats", "dairy", "pulses", "income_elasticity"))
  )

while(i != nrow(exx_par_t)+1 && j != ncol(exx_par_t) ){
  
  for(j in 1:ncol(exx_par_t)){
    temp[i,j] <- paste(signif(exx_par[i,j], 4), calculate_pValue(exx_par_t[i,j], 156)[2], sep="")
    j <- j+1
  }
  i <- i+1
  j <- 1
}
exx_par_t_sig <- temp # put results into matrix

exx_par_r2 <- matrix(c(signif(smry_dld_marshallian_1$r.squared, 5), signif(smry_dld_marshallian_2$r.squared, 5), signif(smry_dld_marshallian_3$r.squared, 5)), ncol=1)

exx_par_wr2 <- cbind(exx_par_t_sig, exx_par_r2)
dimnames(exx_par_wr2) <- list(c("meats_lnq1", "dairy_lnq2", "pulses_lnq3"), c("meats", "dairy", "pulses", "income_elasticity", "R-squared"))

```
  
```{r, echo=FALSE, message=FALSE, fig.height=3, cache=TRUE}

dld_1_formula <- formula(q1 ~ p1+ p2+ p3+ X)
dld_2_formula <- formula(q2 ~ p1+ p2+ p3+ X)
dld_3_formula <- formula(q3 ~ p1+ p2+ p3+ X)

xdat <- Dta_sub_log[,c(1:3, 7)]
ydat_1 <- Dta_sub_log$q1
ydat_2 <- Dta_sub_log$q2
ydat_3 <- Dta_sub_log$q3

dld_1 <- lm(q1 ~ p1+ p2+ p3+ X, data=Dta_sub_log, x=TRUE, y=TRUE)
dld_2 <- lm(q2 ~ p1+ p2+ p3+ X, data=Dta_sub_log, x=TRUE, y=TRUE)
dld_3 <- lm(q3 ~ p1+ p2+ p3+ X, data=Dta_sub_log, x=TRUE, y=TRUE)

### !!! time consuming code - only run when chaging code !!! ###
### Consistent Model Specification Test
# cmsTest_1 <- npcmstest(model=dld_1, xdat=xdat, ydat = ydat_1)
# cmsTest_2 <- npcmstest(model=dld_2, xdat=xdat, ydat = ydat_2)
# cmsTest_3 <- npcmstest(model=dld_3, xdat=xdat, ydat = ydat_3)
# 
# saveRDS(cmsTest_1, file= "Equaddata/cmsTest_1.rds")
# saveRDS(cmsTest_2, file= "Equaddata/cmsTest_2.rds")
# saveRDS(cmsTest_3, file= "Equaddata/cmsTest_3.rds")

cmsTest_1 <- readRDS("Equaddata/cmsTest_1.rds")
cmsTest_2 <- readRDS("Equaddata/cmsTest_2.rds")
cmsTest_3 <- readRDS("Equaddata/cmsTest_3.rds")

### !!! time consuming code - only run when chaging code !!! ###
### bandwidth calculations ###
# bw.all_1 <- npregbw(formula = dld_1_formula, regtype = "ll", bwmethod = "cv.aic", bwtype="fixed", ckertype= "gaussian", data = Dta_sub_log)
# bw.all_2 <- npregbw(formula = dld_2_formula, regtype = "ll", bwmethod = "cv.aic", bwtype="fixed", ckertype= "gaussian", data = Dta_sub_log)
# bw.all_3 <- npregbw(formula = dld_3_formula, regtype = "ll", bwmethod = "cv.aic", bwtype="fixed", ckertype= "gaussian", data = Dta_sub_log)
# 
# saveRDS(bw.all_1, file= "Equaddata/bw.all_1.rds")
# saveRDS(bw.all_2, file= "Equaddata/bw.all_2.rds")
# saveRDS(bw.all_3, file= "Equaddata/bw.all_3.rds")
# saveRDS(bw.all_1, file= "Equaddata/bw.all_1.1.rds")
# saveRDS(bw.all_2, file= "Equaddata/bw.all_2.1.rds")
# saveRDS(bw.all_3, file= "Equaddata/bw.all_3.1.rds")

# bw.all_1 <- readRDS("Equaddata/bw.all_1.rds")
# bw.all_2 <- readRDS("Equaddata/bw.all_2.rds")
# bw.all_3 <- readRDS("Equaddata/bw.all_3.rds")
bw.all_1 <- readRDS("Equaddata/bw.all_1.1.rds")
bw.all_2 <- readRDS("Equaddata/bw.all_2.1.rds")
bw.all_3 <- readRDS("Equaddata/bw.all_3.1.rds")

# summary(bw.all_1)
# summary(bw.all_2)
# summary(bw.all_3)

model_1.np <- npreg(bws = bw.all_1, gradients = TRUE, residuals = TRUE)
model_2.np <- npreg(bws = bw.all_2, gradients = TRUE, residuals = TRUE)
model_3.np <- npreg(bws = bw.all_3, gradients = TRUE, residuals = TRUE)

# summary(model_1.np)
# summary(model_2.np)
# summary(model_3.np)

### !!! time consuming code - only run when chaging code !!! ###
#Kernel Regression Significance Test
# KRS_test_1 <- npsigtest(model_1.np)
# KRS_test_2 <- npsigtest(model_2.np)
# KRS_test_3 <- npsigtest(model_3.np)
# 
# saveRDS(KRS_test_1, file= "Equaddata/KRS_test_1.rds")
# saveRDS(KRS_test_2, file= "Equaddata/KRS_test_2.rds")
# saveRDS(KRS_test_3, file= "Equaddata/KRS_test_3.rds")
# saveRDS(KRS_test_1, file= "Equaddata/KRS_test_1.1.rds")
# saveRDS(KRS_test_2, file= "Equaddata/KRS_test_2.1.rds")
# saveRDS(KRS_test_3, file= "Equaddata/KRS_test_3.1.rds")

# KRS_test_1 <- readRDS("Equaddata/KRS_test_1.rds")
# KRS_test_2 <- readRDS("Equaddata/KRS_test_2.rds")
# KRS_test_3 <- readRDS("Equaddata/KRS_test_3.rds")
KRS_test_1.1 <- readRDS("Equaddata/KRS_test_1.1.rds")
KRS_test_2.1 <- readRDS("Equaddata/KRS_test_2.1.rds")
KRS_test_3.1 <- readRDS("Equaddata/KRS_test_3.1.rds")

e1x_np <- list()
e1x_np$e11_np <- mean(model_1.np$grad[,1])
e1x_np$e12_np <- mean(model_1.np$grad[,2])
e1x_np$e13_np <- mean(model_1.np$grad[,3])
e1x_np$eta1_np <- mean(model_1.np$grad[,4])

e1x_np_t <- list()
e1x_np_t$e11_np_t <- mean(model_1.np$grad[,1])/mean(model_1.np$gerr[,1])
e1x_np_t$e12_np_t <- mean(model_1.np$grad[,2])/mean(model_1.np$gerr[,2])
e1x_np_t$e13_np_t <- mean(model_1.np$grad[,3])/mean(model_1.np$gerr[,3])
e1x_np_t$eta1_np_t <- mean(model_1.np$grad[,4])/mean(model_1.np$gerr[,4])

e2x_np <- list()
e2x_np$e21_np <- mean(model_2.np$grad[,1])
e2x_np$e22_np <- mean(model_2.np$grad[,2])
e2x_np$e23_np <- mean(model_2.np$grad[,3])
e2x_np$eta2_np <- mean(model_2.np$grad[,4])

e2x_np_t <- list()
e2x_np_t$e21_np_t <- mean(model_2.np$grad[,1])/mean(model_2.np$gerr[,1])
e2x_np_t$e22_np_t <- mean(model_2.np$grad[,2])/mean(model_2.np$gerr[,2])
e2x_np_t$e23_np_t <- mean(model_2.np$grad[,3])/mean(model_2.np$gerr[,3])
e2x_np_t$eta2_np_t <- mean(model_2.np$grad[,4])/mean(model_2.np$gerr[,4])

e3x_np <- list()
e3x_np$e31_np <- mean(model_3.np$grad[,1])
e3x_np$e32_np <- mean(model_3.np$grad[,2])
e3x_np$e33_np <- mean(model_3.np$grad[,3])
e3x_np$eta3_np <- mean(model_3.np$grad[,4])

# par(mfrow=c(1,3))
# plot(density(model_1.np$grad[,1]), sub = "Meats - own price elasticity", main= "", xlim=c(-5,5))
# plot(density(model_2.np$grad[,2]), sub = "Dairy - own price elasticity", main= "",xlim=c(-5,5))
# plot(density(model_3.np$grad[,3]), sub = "Pulses - own price elasticity", main= "", xlim=c(-5,5))
# 
# plot(density(model_1.np$grad[,4]), sub = "Meats - income elasticity", main= "", xlim=c(-5,5))
# plot(density(model_2.np$grad[,4]), sub = "Dairy - income elasticity", main= "",xlim=c(-5,5))
# plot(density(model_3.np$grad[,4]), sub = "Pulses - income elasticity", main= "", xlim=c(-5,5))

e3x_np_t <- list()
e3x_np_t$e31_np_t <- mean(model_3.np$grad[,1])/mean(model_3.np$gerr[,1])
e3x_np_t$e32_np_t <- mean(model_3.np$grad[,2])/mean(model_3.np$gerr[,2])
e3x_np_t$e33_np_t <- mean(model_3.np$grad[,3])/mean(model_3.np$gerr[,3])
e3x_np_t$eta3_np_t <- mean(model_3.np$grad[,4])/mean(model_3.np$gerr[,4])

exx_np <- matrix(unlist(c(e1x_np, e2x_np, e3x_np)), nrow = 3, byrow = TRUE)
exx_np_r2 <- matrix(c(signif(model_1.np$R2, 5), signif(model_2.np$R2, 5), signif(model_3.np$R2, 5)), ncol=1)

exx_np_t <- matrix(unlist(c(e1x_np_t, e2x_np_t, e3x_np_t)), nrow = 3, byrow = TRUE)

##############################################################################
### DLD_np - calculate signif on Marshallian demand elasticities matrix    ###
##############################################################################
i <- 1
j <- 1
temp <- matrix(nrow=3, ncol = 4, 
  dimnames = list(c("meats_lnq1", "dairy_lnq2", "pulses_lnq3"), c("meats", "dairy", "pulses", "income_elasticity"))
  )

while(i != nrow(exx_np_t)+1 && j != ncol(exx_np_t) ){
  
  for(j in 1:ncol(exx_np_t)){
    temp[i,j] <- paste(signif(exx_np[i,j], 4), calculate_pValue(exx_np_t[i,j], 156)[2], sep="")
    j <- j+1
  }
  i <- i+1
  j <- 1
}
exx_np_t_sig <- temp # put results into matrix

exx_np_wr2 <- cbind(exx_np_t_sig, exx_np_r2)
dimnames(exx_np_wr2) <- list(c("meats_lnq1", "dairy_lnq2", "pulses_lnq3"), c("meats", "dairy", "pulses", "income_elasticity", "R-squared"))


#########################################
### Full AIDS Model empirical results ###
#########################################
priceNames <- c("p1", "p2", "p3")
shareNames <- c("w1", "w2", "w3")


AIDS_estResult_constraint <- aidsEst( priceNames, shareNames, "X",
                       data = Dta_sub,
                       method = "IL", ILmaxiter = 1000, ILtol = 1e-10,
                       priceIndex = "S", hom = TRUE, sym = TRUE)

AIDS_estResult_no_constraint <- aidsEst( priceNames, shareNames, "X",
                       data = Dta_sub,
                       method = "IL", ILmaxiter = 1000, ILtol = 1e-10,
                       priceIndex = "S", hom = FALSE, sym = FALSE)

#summary(AIDS_estResult_constraint)
#summary(AIDS_estResult_no_constraint)

###########################################
### Full AIDS elasticities and t-values ###
###########################################
AIDS_elas_M_H_income_constraint <- elas( AIDS_estResult_constraint, method = "AIDS" )
AIDS_elas_M_H_income_no_constraint <- elas( AIDS_estResult_no_constraint, method = "AIDS" )

AIDS_elas_M_constraint <- matrix(AIDS_elas_M_H_income_constraint$marshall, nrow = 3)
AIDS_elas_income_constraint <- matrix(AIDS_elas_M_H_income_constraint$exp, nrow = 3)
AIDS_elas_M_income_constraint <- cbind(AIDS_elas_M_constraint, AIDS_elas_income_constraint)

AIDS_elas_M_constraint_t <- matrix(AIDS_elas_M_H_income_constraint$marshallTval, nrow = 3)
AIDS_elas_income_constraint_t <- matrix(AIDS_elas_M_H_income_constraint$expTval, nrow = 3)
AIDS_elas_M_income_constraint_t <- cbind(AIDS_elas_M_constraint_t, AIDS_elas_income_constraint_t)

#################################################################################
### Full AIDS - calculate signif on Marshallian demand elasticities matrix    ###
#################################################################################
i <- 1
j <- 1
temp <- matrix(nrow=3, ncol = 4, 
  dimnames = list(c("meats_lnq1", "dairy_lnq2", "pulses_lnq3"), c("meats", "dairy", "pulses", "income_elasticity"))
  )

while(i != nrow(AIDS_elas_M_income_constraint_t)+1 && j != ncol(AIDS_elas_M_income_constraint_t) ){
  
  for(j in 1:ncol(AIDS_elas_M_income_constraint_t)){
    temp[i,j] <- paste(signif(AIDS_elas_M_income_constraint[i,j], 4), calculate_pValue(AIDS_elas_M_income_constraint_t[i,j], 156)[2], sep="")
    j <- j+1
  }
  i <- i+1
  j <- 1
}
AIDS_elas_M_income_constraint_t_sig <- temp # put results into matrix

AIDS_elas_M_income_constraint_r2 <- matrix(signif(AIDS_estResult_constraint$r2, 5), ncol=1, dimnames = list(c("meats_lnq1", "dairy_lnq2", "pulses_lnq3"), c("R-squared")))

AIDS_elas_M_income_constraint_t_sig_wr2 <- cbind(AIDS_elas_M_income_constraint_t_sig, 
                                                 AIDS_elas_M_income_constraint_r2)




###     ###
# weights <- c( lnp1_catfish=mean(Dta_sub$lnp1_catfish), lnp2_crawfish=mean(Dta_sub$lnp2_crawfish), lnp3_clams=mean(Dta_sub$lnp3_clams), lnp4_shrimp=mean(Dta_sub$lnp4_shrimp), lnp5_tilapia=mean(Dta_sub$lnp5_tilapia), lnp6_salmon=mean(Dta_sub$lnp6_salmon))
# 
# weights <- weights/sum(weights)
# 
# est1_np <- npregHom(yName = "lnq1_catfish", xNames = c("lnp1_catfish", "lnp2_crawfish", "lnp3_clams", "lnp4_shrimp", "lnp5_tilapia", "lnp6_salmon", "lnX"), data=Dta_sub, homWeights = weights)
# 
# est2_np <- npregHom(yName = "lnq2_crawfish", xNames = c("lnp1_catfish", "lnp2_crawfish", "lnp3_clams", "lnp4_shrimp", "lnp5_tilapia", "lnp6_salmon", "lnX"), data=Dta_sub, homWeights = weights)
# 
# e1x_np <- list()
# e1x_np$e11 <- mean(est1_np$est$grad[,1])
# e1x_np$e12 <- mean(est1_np$est$grad[,2])
# e1x_np$e13 <- mean(est1_np$est$grad[,3])
# e1x_np$e14 <- mean(est1_np$est$grad[,4])
# e1x_np$e15 <- mean(est1_np$est$grad[,5])
# e1x_np$e16 <- mean(est1_np$est$grad[,6])
# 
# e2x_np <- list()
# e2x_np$e21 <- mean(est2_np$est$grad[,1])
# e2x_np$e22 <- mean(est2_np$est$grad[,2])
# e2x_np$e23 <- mean(est2_np$est$grad[,3])
# e2x_np$e24 <- mean(est2_np$est$grad[,4])
# e2x_np$e25 <- mean(est2_np$est$grad[,5])
# e2x_np$e26 <- mean(est2_np$est$grad[,6])
# 

```
  
# Introduction
  
Emperical demand analysis has been dominated by the use of parametric functional form models since the appearance of the Working-Lesser Model in the 1940's [@Working:1943wb] [@Leser:1963vd]. A 2015 paper by Clements and Gao [@Clements:2015wj] provide a citation count of journal articles related to four popular parametric demand models (Linear Expenditure, Rotterdam, Translog and Almost Ideal Demand). The authors considered multiple time periods over the date range of 1974-2013. Their results clearly show an upward trajectory for all four parametric demand models with the Linear Expenditure and Almost Ideal Demand (AIDS) models having the highest citation counts, both in the multiple thousands of citations. This result suggests that parametric demand modeling is still very much a hotly researched area.  
  
While parametric form models have clearly dominated the research landscape there are other empirical techniques available to evalaute consumer expenditure datasets. The goal of empirical demand analysis is to answer basic questions about consumer behavior and in many cases, use these answers to design and implement policy (government) or imporove some aspect of business design to increase market exposure/profits (private industry). Beyond these practical objectives Hal Varian [@Varian:1982wa] suggests that, given a consumers expenditure dataset the demand analyst should ask four basic questions concerning the consumers behavior,  
  
> 
(1) Consistency? Is the observerd data consistent with the utility maximizing model?
(2) Structure? Is the observed data consistent with a utility function with some special structure?
(3) Recoverability? How can the underlying utility function be recovered?
(4) Extrapolation? How can we forecast behavior in other circumstances?
  
With these goals in mind economists are constantly refining their tools to improve their estimates of consumer behavior.  
  
Nonparametric economic analysis has been a growing field of study over the past several decades. Many new techniques have been developed within a theoretical framework. However, despite the rapid growth of theoretical results, nonparametric applied research has lagged considerably. This may be due in part to the advanced mathematical and statistical exposition presented in many nonparametric research papers. It is often difficult for economist, while well trained in advanced mathematical techinques, to fully grasp the significance and translate from purley theoretical results into applied research. Theoretical researchers working within the field of mathematics and statistics often fail to consider applied economic problems and applied economist are not exploring these newly developed theoretical techinques and refining the theory once they have touched real world data. In addition, until recently, nonparametric techniques were substantially more computationally intensive compared to their paramteric counterparts. While the previous statement would seem to imply that nonparametric tecniques have now become computationally less intesive, in reality it is our computer hardware and programing techniques which have improved to the point where nonparametric analysis has become a viable alternative, especially where parametric techniques fall short. These improvemnts in computer hardware have effectivly opened an area of research which, until recently, had remained closed.  
  
Regression analysis is the workhorse technique employed in econometrics. However, in linear regression it is assumed the regressors enter the conditional mean in a linear fashion and each regressor is independent of the other which is often a violation of the data under study. Even when we use nonlinear regression techniques we often still assume we know the functional form for the data generating process (DGP).  
  
The single largest potiential issue with using a parametric form model to evaluate consumer demand is the prior imposition of functional form on the demand model. If a demand system is well represented by a functional form and the econometric model is correctly specified with theoretical assumptions met then a parametric estimator is both consistent and effiecient. In fact if the previous criteria are met parametric regression is superior in just about every way. However, these requirements are excessivly difficult to satisfy when used in the wild to gain insight about real world data and thus the parametric approach can be seroiusly flawed and worst, can lead the researcher to faulty conclusions which can have serious repercussions if used to implement policy.  
  
Nonparametric modelling affords many advantages over their parametric counterparts. Primary amoung them is the nonparametric models ability to help us uncover a more accurate representation of the unknown function conditioned on the actual data in hand. In section 2 I will explore the methods and models used to estimate a nonparametric regression. In nonparametric analysis the critical step to success is choosing a correct bandwidth estimator so some time will be spend exploring several optional available. I will also explore the two most popular kernel regression methods in current use, Local Constant Least Squares (LCLS) and Local Linear Least Squares (LLLS). I will summarize the data and quickly review the parametric models being used for comparison purposes. Section 3 explores the results obtained from the nonparametric kernel regression and compares these results with the parametric form models used on the same dataset. Section 4 concludes and details direction for additional studies.  

# Methods, Models and Data 
  





  
```{r, echo=FALSE, message=FALSE, results='asis', cache=TRUE}

stargazer(Dta_sub, header=F, type="latex", summary = TRUE, font.size = "small", notes= c("good 1 = meats", "good 2 = dairy", "good 3 = beans"), notes.align= "l", flip = F, float = T, float.env = "table", title="Summary Statistics")

```


# Results and Discussion
  
```{r, echo=FALSE, message=FALSE, results='asis', cache=TRUE}

stargazer(exx_par_wr2, header=F, type="latex", summary = FALSE, font.size = "small", notes= c("***Significant at the 1 percent level,", "**Significant at the 5 percent level,", "*Significant at the 10 percent level."), notes.align= "r", flip = F, float = T, float.env = "table", title="Parametric - Double Log Demand Model")

stargazer(exx_np_wr2, header=F, type="latex", summary = FALSE, font.size = "small", notes= c("***Significant at the 1 percent level,", "**Significant at the 5 percent level,", "*Significant at the 10 percent level."), notes.align= "r", flip = F, float = T, float.env = "table", title="Nonparametric Regression using Gaussian Kernel")

stargazer(AIDS_elas_M_income_constraint_t_sig_wr2, header=F, type="latex", summary = FALSE, font.size = "small", notes= c("***Significant at the 1 percent level,", "**Significant at the 5 percent level,", "*Significant at the 10 percent level."), notes.align= "r", flip = F, float = T, float.env = "table", title="Full AIDS - Marshallian")

#stargazer(AIDS_a_b_eta, header=F, type="latex", summary = FALSE, font.size = "small", notes= c("***Significant at the 1 percent level,", "**Significant at the 5 percent level,", "*Significant at the 10 percent level."), notes.align= "r", flip = F, float = T, float.env = "table", title="Full AIDS - Alpha, and Beta parameter estimates, Average Budget Shares, and Income Elasticities")

```
  


# Conclusions
  





\newpage

# References
