---
title: "Nonparametric Demand Analysis"
subtitle: "Draft - not for publication"
author: "Matthew Aaron Looney"
date: "08/22/2017"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: no
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '4'
header-includes:
- \usepackage{graphicx}
- \usepackage{rotating}
- \usepackage{longtable}
- \usepackage{amssymb,amsmath}
- \usepackage{dcolumn}
- \usepackage{setspace}\doublespacing
- \usepackage{mathrsfs}
- \usepackage{eso-pic,graphicx,transparent}

abstract: "Nonparametric econometric analysis has been a growing field of study over the past several decades. Many new techniques have been developed within a theoretical framework. However, despite the rapid growth of theoretical results, nonparametric applied research has lagged considerably. This paper employs a nonparametric regression analysis within the context of demand theory. Data on prices and quantites of three commodities (meats, dairy products and beans) from the 2006 Ecuadorian consumer expenditure survey will be evaluated to derive Marshallian price and income elasticities. The nonparametric results will be compared with standard parametric demand analysis tools such as the Log-Log demand model and the Almost Ideal Demand system to guage the effectivness of using nonparametric techniques to estimate demand elasticities."

bibliography: np_demand_paper.bib
csl: computational-economics.csl
link-citations: true
fontsize: 12pt
geometry: margin = 1.25in
---

<!-- \AddToShipoutPictureFG{ -->
<!--   \AtPageCenter{% or \AtTextCenter -->
<!--     \makebox[0pt]{\rotatebox[origin=c]{45}{% -->
<!--       \scalebox{5}{\texttransparent{0.1}{DRAFT}}% -->
<!--     }} -->
<!--   } -->
<!-- } -->

```{r House Keeping, echo=FALSE, message=FALSE, cache=TRUE}

rm(list=ls())

# randNum <- round(runif(1, 1, 1e8)) # !!! RUN ONLY ONCE TO GENERATE RN for seed !!!
# randNum
set.seed(38532842) # all sequential simulations will start from this seed

library(readxl)
library(stargazer)
library(systemfit)
library(np)
library(micEconAids)

options(np.messages=FALSE) 
#options(digits=5)

setwd("~/Google Drive/digitalLibrary/*AAEC 6310 Demand and Price Analysis/DLDemand_nonparametric")

source("~/Google Drive/digitalLibrary/*AAEC 6310 Demand and Price Analysis/demandLab1/calculate_pValue.R")
source("~/Google Drive/digitalLibrary/*AAEC 6310 Demand and Price Analysis/demandLab1/calculate_tValues.R")

```

```{r Import & data manip, echo=FALSE, message=FALSE, cache=TRUE}

Dta <- read.csv("/Users/malooney/Google Drive/digitalLibrary/*Econometrics2/Lab3/protein.csv")
Dta_sub <- data.frame(obs=1:nrow(Dta))

Dta_sub$p1 <- Dta$pmeats1
Dta_sub$p2 <- Dta$pdairy1
Dta_sub$p3 <- Dta$ppulses1

Dta_sub$q1 <- Dta$qmeats
Dta_sub$q2 <- Dta$qdairy
Dta_sub$q3 <- Dta$qpulses

Dta_sub$p1q1 <- Dta$qmeats*Dta$pmeats1
Dta_sub$p2q2 <- Dta$qdairy*Dta$pdairy1
Dta_sub$p3q3 <- Dta$ppulses1*Dta$qpulses

Dta_sub$X <- Dta_sub$p1q1 + Dta_sub$p2q2 + Dta_sub$p3q3

Dta_sub$w1 <- Dta_sub$p1q1 / Dta_sub$X
Dta_sub$w2 <- Dta_sub$p2q2 / Dta_sub$X
Dta_sub$w3 <- Dta_sub$p3q3 / Dta_sub$X

Dta_sub_log <- Dta_sub
Dta_sub_log <- log(Dta_sub[,c(2:7,11)])

```

```{r Single Equation Model Estimation using Log-Log Functional Form, echo=FALSE, message=FALSE, cache=TRUE}

# Single Equation Model Estimation using Log-Log Functional Form
# Estimate Marshallian elasticities - unrestricted

dld_marshallian_1.lm <- lm(q1 ~ p1+ p2+ p3+ X, data=Dta_sub_log)
dld_marshallian_2.lm <- lm(q2 ~ p1+ p2+ p3+ X, data=Dta_sub_log)
dld_marshallian_3.lm <- lm(q3 ~ p1+ p2+ p3+ X, data=Dta_sub_log)

smry_dld_marshallian_1 <- summary(dld_marshallian_1.lm)
smry_dld_marshallian_2 <- summary(dld_marshallian_2.lm)
smry_dld_marshallian_3 <- summary(dld_marshallian_3.lm)

exx_par <- matrix(unlist(c(coef(dld_marshallian_1.lm)[2:5], coef(dld_marshallian_2.lm)[2:5], coef(dld_marshallian_3.lm)[2:5])), nrow = 3, byrow = TRUE)

exx_par_t <- matrix(
  unlist(c( 
  coef(dld_marshallian_1.lm)[2:5]/sqrt(diag(vcov(dld_marshallian_1.lm)[-1,-1])), 
  coef(dld_marshallian_2.lm)[2:5]/sqrt(diag(vcov(dld_marshallian_2.lm)[-1,-1])), 
  coef(dld_marshallian_3.lm)[2:5]/sqrt(diag(vcov(dld_marshallian_3.lm)[-1,-1]))
  )), 
  nrow = 3, byrow = TRUE)

######################################################################################
### DLD_parametric - calculate signif on Marshallian demand elasticities matrix    ###
######################################################################################
i <- 1
j <- 1
temp <- matrix(nrow=3, ncol = 4, 
  dimnames = list(c("meats_lnq1", "dairy_lnq2", "pulses_lnq3"), c("meats", "dairy", "pulses", "income_elasticity"))
  )

while(i != nrow(exx_par_t)+1 && j != ncol(exx_par_t) ){
  
  for(j in 1:ncol(exx_par_t)){
    temp[i,j] <- paste(signif(exx_par[i,j], 4), calculate_pValue(exx_par_t[i,j], 156)[2], sep="")
    j <- j+1
  }
  i <- i+1
  j <- 1
}
exx_par_t_sig <- temp # put results into matrix

exx_par_r2 <- matrix(c(signif(smry_dld_marshallian_1$r.squared, 5), signif(smry_dld_marshallian_2$r.squared, 5), signif(smry_dld_marshallian_3$r.squared, 5)), ncol=1)

exx_par_wr2 <- cbind(exx_par_t_sig, exx_par_r2)
dimnames(exx_par_wr2) <- list(c("meats_lnq1", "dairy_lnq2", "pulses_lnq3"), c("meats", "dairy", "pulses", "income_elasticity", "R-squared"))

```
  
```{r, echo=FALSE, message=FALSE, fig.height=3, cache=TRUE}

dld_1_formula <- formula(q1 ~ p1+ p2+ p3+ X)
dld_2_formula <- formula(q2 ~ p1+ p2+ p3+ X)
dld_3_formula <- formula(q3 ~ p1+ p2+ p3+ X)

xdat <- Dta_sub_log[,c(1:3, 7)]
ydat_1 <- Dta_sub_log$q1
ydat_2 <- Dta_sub_log$q2
ydat_3 <- Dta_sub_log$q3

dld_1 <- lm(q1 ~ p1+ p2+ p3+ X, data=Dta_sub_log, x=TRUE, y=TRUE)
dld_2 <- lm(q2 ~ p1+ p2+ p3+ X, data=Dta_sub_log, x=TRUE, y=TRUE)
dld_3 <- lm(q3 ~ p1+ p2+ p3+ X, data=Dta_sub_log, x=TRUE, y=TRUE)

### !!! time consuming code - only run when chaging code !!! ###
### Consistent Model Specification Test
# cmsTest_1 <- npcmstest(model=dld_1, xdat=xdat, ydat = ydat_1)
# cmsTest_2 <- npcmstest(model=dld_2, xdat=xdat, ydat = ydat_2)
# cmsTest_3 <- npcmstest(model=dld_3, xdat=xdat, ydat = ydat_3)
# 
# saveRDS(cmsTest_1, file= "Equaddata/cmsTest_1.rds")
# saveRDS(cmsTest_2, file= "Equaddata/cmsTest_2.rds")
# saveRDS(cmsTest_3, file= "Equaddata/cmsTest_3.rds")

cmsTest_1 <- readRDS("Equaddata/cmsTest_1.rds")
cmsTest_2 <- readRDS("Equaddata/cmsTest_2.rds")
cmsTest_3 <- readRDS("Equaddata/cmsTest_3.rds")

### !!! time consuming code - only run when chaging code !!! ###
### bandwidth calculations ###
# bw.all_1 <- npregbw(formula = dld_1_formula, regtype = "ll", bwmethod = "cv.aic", bwtype="fixed", ckertype= "gaussian", data = Dta_sub_log)
# bw.all_2 <- npregbw(formula = dld_2_formula, regtype = "ll", bwmethod = "cv.aic", bwtype="fixed", ckertype= "gaussian", data = Dta_sub_log)
# bw.all_3 <- npregbw(formula = dld_3_formula, regtype = "ll", bwmethod = "cv.aic", bwtype="fixed", ckertype= "gaussian", data = Dta_sub_log)
# 
# saveRDS(bw.all_1, file= "Equaddata/bw.all_1.rds")
# saveRDS(bw.all_2, file= "Equaddata/bw.all_2.rds")
# saveRDS(bw.all_3, file= "Equaddata/bw.all_3.rds")
# saveRDS(bw.all_1, file= "Equaddata/bw.all_1.1.rds")
# saveRDS(bw.all_2, file= "Equaddata/bw.all_2.1.rds")
# saveRDS(bw.all_3, file= "Equaddata/bw.all_3.1.rds")

# bw.all_1 <- readRDS("Equaddata/bw.all_1.rds")
# bw.all_2 <- readRDS("Equaddata/bw.all_2.rds")
# bw.all_3 <- readRDS("Equaddata/bw.all_3.rds")
bw.all_1 <- readRDS("Equaddata/bw.all_1.1.rds")
bw.all_2 <- readRDS("Equaddata/bw.all_2.1.rds")
bw.all_3 <- readRDS("Equaddata/bw.all_3.1.rds")

# summary(bw.all_1)
# summary(bw.all_2)
# summary(bw.all_3)

model_1.np <- npreg(bws = bw.all_1, gradients = TRUE, residuals = TRUE)
model_2.np <- npreg(bws = bw.all_2, gradients = TRUE, residuals = TRUE)
model_3.np <- npreg(bws = bw.all_3, gradients = TRUE, residuals = TRUE)

# summary(model_1.np)
# summary(model_2.np)
# summary(model_3.np)

### !!! time consuming code - only run when chaging code !!! ###
#Kernel Regression Significance Test
# KRS_test_1 <- npsigtest(model_1.np)
# KRS_test_2 <- npsigtest(model_2.np)
# KRS_test_3 <- npsigtest(model_3.np)
# 
# saveRDS(KRS_test_1, file= "Equaddata/KRS_test_1.rds")
# saveRDS(KRS_test_2, file= "Equaddata/KRS_test_2.rds")
# saveRDS(KRS_test_3, file= "Equaddata/KRS_test_3.rds")
# saveRDS(KRS_test_1, file= "Equaddata/KRS_test_1.1.rds")
# saveRDS(KRS_test_2, file= "Equaddata/KRS_test_2.1.rds")
# saveRDS(KRS_test_3, file= "Equaddata/KRS_test_3.1.rds")

# KRS_test_1 <- readRDS("Equaddata/KRS_test_1.rds")
# KRS_test_2 <- readRDS("Equaddata/KRS_test_2.rds")
# KRS_test_3 <- readRDS("Equaddata/KRS_test_3.rds")
KRS_test_1.1 <- readRDS("Equaddata/KRS_test_1.1.rds")
KRS_test_2.1 <- readRDS("Equaddata/KRS_test_2.1.rds")
KRS_test_3.1 <- readRDS("Equaddata/KRS_test_3.1.rds")

e1x_np <- list()
e1x_np$e11_np <- mean(model_1.np$grad[,1])
e1x_np$e12_np <- mean(model_1.np$grad[,2])
e1x_np$e13_np <- mean(model_1.np$grad[,3])
e1x_np$eta1_np <- mean(model_1.np$grad[,4])

e1x_np_t <- list()
e1x_np_t$e11_np_t <- mean(model_1.np$grad[,1])/mean(model_1.np$gerr[,1])
e1x_np_t$e12_np_t <- mean(model_1.np$grad[,2])/mean(model_1.np$gerr[,2])
e1x_np_t$e13_np_t <- mean(model_1.np$grad[,3])/mean(model_1.np$gerr[,3])
e1x_np_t$eta1_np_t <- mean(model_1.np$grad[,4])/mean(model_1.np$gerr[,4])

e2x_np <- list()
e2x_np$e21_np <- mean(model_2.np$grad[,1])
e2x_np$e22_np <- mean(model_2.np$grad[,2])
e2x_np$e23_np <- mean(model_2.np$grad[,3])
e2x_np$eta2_np <- mean(model_2.np$grad[,4])

e2x_np_t <- list()
e2x_np_t$e21_np_t <- mean(model_2.np$grad[,1])/mean(model_2.np$gerr[,1])
e2x_np_t$e22_np_t <- mean(model_2.np$grad[,2])/mean(model_2.np$gerr[,2])
e2x_np_t$e23_np_t <- mean(model_2.np$grad[,3])/mean(model_2.np$gerr[,3])
e2x_np_t$eta2_np_t <- mean(model_2.np$grad[,4])/mean(model_2.np$gerr[,4])

e3x_np <- list()
e3x_np$e31_np <- mean(model_3.np$grad[,1])
e3x_np$e32_np <- mean(model_3.np$grad[,2])
e3x_np$e33_np <- mean(model_3.np$grad[,3])
e3x_np$eta3_np <- mean(model_3.np$grad[,4])

# par(mfrow=c(1,3))
# plot(density(model_1.np$grad[,1]), sub = "Meats - own price elasticity", main= "", xlim=c(-5,5))
# plot(density(model_2.np$grad[,2]), sub = "Dairy - own price elasticity", main= "",xlim=c(-5,5))
# plot(density(model_3.np$grad[,3]), sub = "Beans - own price elasticity", main= "", xlim=c(-5,5))
# 
# plot(density(model_1.np$grad[,4]), sub = "Meats - income elasticity", main= "", xlim=c(-5,5))
# plot(density(model_2.np$grad[,4]), sub = "Dairy - income elasticity", main= "",xlim=c(-5,5))
# plot(density(model_3.np$grad[,4]), sub = "Beans - income elasticity", main= "", xlim=c(-5,5))

e3x_np_t <- list()
e3x_np_t$e31_np_t <- mean(model_3.np$grad[,1])/mean(model_3.np$gerr[,1])
e3x_np_t$e32_np_t <- mean(model_3.np$grad[,2])/mean(model_3.np$gerr[,2])
e3x_np_t$e33_np_t <- mean(model_3.np$grad[,3])/mean(model_3.np$gerr[,3])
e3x_np_t$eta3_np_t <- mean(model_3.np$grad[,4])/mean(model_3.np$gerr[,4])

exx_np <- matrix(unlist(c(e1x_np, e2x_np, e3x_np)), nrow = 3, byrow = TRUE)
exx_np_r2 <- matrix(c(signif(model_1.np$R2, 5), signif(model_2.np$R2, 5), signif(model_3.np$R2, 5)), ncol=1)

exx_np_t <- matrix(unlist(c(e1x_np_t, e2x_np_t, e3x_np_t)), nrow = 3, byrow = TRUE)

##############################################################################
### DLD_np - calculate signif on Marshallian demand elasticities matrix    ###
##############################################################################
i <- 1
j <- 1
temp <- matrix(nrow=3, ncol = 4, 
  dimnames = list(c("meats_lnq1", "dairy_lnq2", "pulses_lnq3"), c("meats", "dairy", "pulses", "income_elasticity"))
  )

while(i != nrow(exx_np_t)+1 && j != ncol(exx_np_t) ){
  
  for(j in 1:ncol(exx_np_t)){
    temp[i,j] <- paste(signif(exx_np[i,j], 4), calculate_pValue(exx_np_t[i,j], 156)[2], sep="")
    j <- j+1
  }
  i <- i+1
  j <- 1
}
exx_np_t_sig <- temp # put results into matrix

exx_np_wr2 <- cbind(exx_np_t_sig, exx_np_r2)
dimnames(exx_np_wr2) <- list(c("meats_lnq1", "dairy_lnq2", "pulses_lnq3"), c("meats", "dairy", "pulses", "income_elasticity", "R-squared"))


#########################################
### Full AIDS Model empirical results ###
#########################################
priceNames <- c("p1", "p2", "p3")
shareNames <- c("w1", "w2", "w3")


AIDS_estResult_constraint <- aidsEst( priceNames, shareNames, "X",
                       data = Dta_sub,
                       method = "IL", ILmaxiter = 1000, ILtol = 1e-10,
                       priceIndex = "S", hom = TRUE, sym = TRUE)

AIDS_estResult_no_constraint <- aidsEst( priceNames, shareNames, "X",
                       data = Dta_sub,
                       method = "IL", ILmaxiter = 1000, ILtol = 1e-10,
                       priceIndex = "S", hom = FALSE, sym = FALSE)

#summary(AIDS_estResult_constraint)
#summary(AIDS_estResult_no_constraint)

###########################################
### Full AIDS elasticities and t-values ###
###########################################
AIDS_elas_M_H_income_constraint <- elas( AIDS_estResult_constraint, method = "AIDS" )
AIDS_elas_M_H_income_no_constraint <- elas( AIDS_estResult_no_constraint, method = "AIDS" )

AIDS_elas_M_constraint <- matrix(AIDS_elas_M_H_income_constraint$marshall, nrow = 3)
AIDS_elas_income_constraint <- matrix(AIDS_elas_M_H_income_constraint$exp, nrow = 3)
AIDS_elas_M_income_constraint <- cbind(AIDS_elas_M_constraint, AIDS_elas_income_constraint)

AIDS_elas_M_constraint_t <- matrix(AIDS_elas_M_H_income_constraint$marshallTval, nrow = 3)
AIDS_elas_income_constraint_t <- matrix(AIDS_elas_M_H_income_constraint$expTval, nrow = 3)
AIDS_elas_M_income_constraint_t <- cbind(AIDS_elas_M_constraint_t, AIDS_elas_income_constraint_t)

#################################################################################
### Full AIDS - calculate signif on Marshallian demand elasticities matrix    ###
#################################################################################
i <- 1
j <- 1
temp <- matrix(nrow=3, ncol = 4, 
  dimnames = list(c("meats_lnq1", "dairy_lnq2", "pulses_lnq3"), c("meats", "dairy", "pulses", "income_elasticity"))
  )

while(i != nrow(AIDS_elas_M_income_constraint_t)+1 && j != ncol(AIDS_elas_M_income_constraint_t) ){
  
  for(j in 1:ncol(AIDS_elas_M_income_constraint_t)){
    temp[i,j] <- paste(signif(AIDS_elas_M_income_constraint[i,j], 4), calculate_pValue(AIDS_elas_M_income_constraint_t[i,j], 156)[2], sep="")
    j <- j+1
  }
  i <- i+1
  j <- 1
}
AIDS_elas_M_income_constraint_t_sig <- temp # put results into matrix

AIDS_elas_M_income_constraint_r2 <- matrix(signif(AIDS_estResult_constraint$r2, 5), ncol=1, dimnames = list(c("meats_lnq1", "dairy_lnq2", "pulses_lnq3"), c("R-squared")))

AIDS_elas_M_income_constraint_t_sig_wr2 <- cbind(AIDS_elas_M_income_constraint_t_sig, 
                                                 AIDS_elas_M_income_constraint_r2)




###     ###
# weights <- c( lnp1_catfish=mean(Dta_sub$lnp1_catfish), lnp2_crawfish=mean(Dta_sub$lnp2_crawfish), lnp3_clams=mean(Dta_sub$lnp3_clams), lnp4_shrimp=mean(Dta_sub$lnp4_shrimp), lnp5_tilapia=mean(Dta_sub$lnp5_tilapia), lnp6_salmon=mean(Dta_sub$lnp6_salmon))
# 
# weights <- weights/sum(weights)
# 
# est1_np <- npregHom(yName = "lnq1_catfish", xNames = c("lnp1_catfish", "lnp2_crawfish", "lnp3_clams", "lnp4_shrimp", "lnp5_tilapia", "lnp6_salmon", "lnX"), data=Dta_sub, homWeights = weights)
# 
# est2_np <- npregHom(yName = "lnq2_crawfish", xNames = c("lnp1_catfish", "lnp2_crawfish", "lnp3_clams", "lnp4_shrimp", "lnp5_tilapia", "lnp6_salmon", "lnX"), data=Dta_sub, homWeights = weights)
# 
# e1x_np <- list()
# e1x_np$e11 <- mean(est1_np$est$grad[,1])
# e1x_np$e12 <- mean(est1_np$est$grad[,2])
# e1x_np$e13 <- mean(est1_np$est$grad[,3])
# e1x_np$e14 <- mean(est1_np$est$grad[,4])
# e1x_np$e15 <- mean(est1_np$est$grad[,5])
# e1x_np$e16 <- mean(est1_np$est$grad[,6])
# 
# e2x_np <- list()
# e2x_np$e21 <- mean(est2_np$est$grad[,1])
# e2x_np$e22 <- mean(est2_np$est$grad[,2])
# e2x_np$e23 <- mean(est2_np$est$grad[,3])
# e2x_np$e24 <- mean(est2_np$est$grad[,4])
# e2x_np$e25 <- mean(est2_np$est$grad[,5])
# e2x_np$e26 <- mean(est2_np$est$grad[,6])
# 

```
  
\newpage
  
# Introduction
  
Emperical demand analysis has been dominated by the use of parametric functional form models since the appearance of the Working-Lesser Model in the 1940's [@Working:1943wb] [@Leser:1963vd]. A 2015 paper by Clements and Gao [@Clements:2015wj] provide a citation count of journal articles related to four popular parametric demand models (Linear Expenditure, Rotterdam, Translog and Almost Ideal Demand). The authors considered multiple time periods over the date range of 1974-2013. Their results clearly show an upward trajectory for all four parametric demand models with the Linear Expenditure and Almost Ideal Demand (AIDS) models having the highest citation counts, both in the multiple thousands of citations. This result suggests that parametric demand modeling is still very much a hotly researched area.  
  
While parametric form models have clearly dominated the research landscape there are other empirical techniques available to evalaute consumer expenditure datasets. The goal of empirical demand analysis is to answer basic questions about consumer behavior and in many cases, use these answers to design and implement policy (government) or imporove some aspect of business design to increase market exposure/profits (private industry). Beyond these practical objectives Hal Varian [@Varian:1982wa] suggests that, given a consumers expenditure dataset the demand analyst should ask four basic questions concerning the consumers behavior,  
  
> 
(1) Consistency? Is the observerd data consistent with the utility maximizing model?
(2) Structure? Is the observed data consistent with a utility function with some special structure?
(3) Recoverability? How can the underlying utility function be recovered?
(4) Extrapolation? How can we forecast behavior in other circumstances?
  
With these goals in mind economists are constantly refining their tools to improve their estimates of consumer behavior.  
  
Nonparametric economic analysis has been a growing field of study over the past several decades. Many new techniques have been developed within a theoretical framework. However, despite the rapid growth of theoretical results, nonparametric applied research has lagged considerably. This may be due in part to the advanced mathematical and statistical exposition presented in many nonparametric research papers. It is often difficult for economist, while well trained in advanced mathematical techinques, to fully grasp the significance and translate from purley theoretical results into applied research. Theoretical researchers working within the field of mathematics and statistics often fail to consider applied economic problems and applied economist are not exploring these newly developed theoretical techinques and refining the theory once they have touched real world data. In addition, until recently, nonparametric techniques were substantially more computationally intensive compared to their paramteric counterparts. While the previous statement would seem to imply that nonparametric tecniques have now become computationally less intesive, in reality it is our computer hardware and programing techniques which have improved to the point where nonparametric analysis has become a viable alternative, especially where parametric techniques fall short. These improvemnts in computer hardware have effectivly opened an area of research which, until recently, had remained closed.  
  
Regression analysis is the workhorse technique employed in econometrics. However, in linear regression it is assumed the regressors enter the conditional mean in a linear fashion and each regressor is independent of the other which is often a violation of the data under study. Even when we use nonlinear regression techniques we often still assume we know the functional form for the data generating process (DGP).  
  
The single largest potiential issue with using a parametric form model to evaluate consumer demand is the prior imposition of functional form on the demand model. If a demand system is well represented by a functional form and the econometric model is correctly specified with theoretical assumptions met then a parametric estimator is both consistent and effiecient. In fact if the previous criteria are met parametric regression is superior in just about every way. However, these requirements are excessivly difficult to satisfy when used in the wild to gain insight about real world data and thus the parametric approach can be seroiusly flawed and worst, can lead the researcher to faulty conclusions which can have serious repercussions if used to implement policy.  
  
Nonparametric modelling affords many advantages over their parametric counterparts. Primary amoung them is the nonparametric models ability to help us uncover a more accurate representation of the unknown function, conditioned on the actual data in hand. In section 2 I will explore the methods and models used to estimate a nonparametric regression. In nonparametric analysis a critical step in acheving a reliable kernel estimator is choosing the correct bandwidth estimator. Some time will be spend exploring several different bandwidth estimators which are prominant in the current literature. I will also explore the two most popular kernel regression methods in current use, Local-Constant Least Squares (LCLS) and Local-Linear Least Squares (LLLS). I will summarize the data and quickly review the parametric models being used for comparison purposes. Section 3 explores the results obtained from the nonparametric kernel regression and compares these results with the parametric form models used on the same dataset. Section 4 concludes and details direction for additional studies.  

# Models, Methods, and Data  
  
In this paper we seek to explore the validity of using nonparametric techniques to uncover estimates of demand elasticities. To this end we use the Log-Log demand system developed by Working [-@Working:1943wb] and Lesser [-@Leser:1963vd]. The benefit of using this type of parametric form is the sheer simplicity of the model. This simplicity allows us to easily compare the effectivness of our nonparametric econometric technique without getting bogged down in complicated estimation considerations or theory constraints. The only theory constraints available to the Log-Log demand model is homogeneity of degree zero in prices and income. However, even though this constraint is available to the Log-Log model we decide to forego all economic theory constraints to make comparability more consistent.  
  
## Log-Log model  
  
Equation 1 shows the Log-Log model specification for the Marshallian own and cross prices elasticities ($e_{ik}$) and the income elasticities ($e_i$). Since we are unable to impose cross equation constraints our study will focus on the own price and income elasticities of this model. While the Hicksian elasticities are of more interest for policy and welfare studies, we focus only on the Marshallian estimates so a direct estimate comparison can be made between the parametric and nonparametric elasticity.  
  
\begin{equation} \label{marshal_unrestricted}
\log ({q_i}) = {\alpha _i} + {e_i}\log (x) + \sum\limits_{k = 1}^n {{e_{ik}}\log ({p_k})} 
\end{equation}
  
It is well known that a correctly specified parametric model is prefered over nonparametric methods. However, we also know that correct model specification prior to study is a near impossible task. It is important to test the parametric model to assess if the functional form is consistent with the DGP. The test we untilize in this study was developed by Hasiao, Li and Racine and is a test of Consistent Model Specification (CMS-test) [@Hsiao:2007uk].
  
## Consistent Model Specification Test  
  
The CMS-test is a kernel based evaluation which tests for correct specification of the parametric form model. A short review of the details of the test are given below.  
  
The null hypothesis is given by,  
  
${H_0}:P[E({y_i}|{x_i}) = m({x_i},\beta )] = 1{\text{ for some }}\beta  \in \mathscr{B}$  
  
where,  
  
$m( \cdot , \cdot )$ is a known function with $\beta$ being a $p \times 1$ vecotr of unknown paramaters,  
  
$\mathscr{B}$ is a compact subset in ${\mathbb{R}^p}$.  
  
The alternative hypothesis is given by,  
  
${H_1}:P[E({y_i}|{x_i}) = m({x_i},\beta )] < 1{\text{ for all }}\beta  \in \mathscr{B}$
  
The test statistic was originally developed by Fan and Li [-@Fan:1996wr]  and Zheng [-@JohnXuZheng:1996uo].  
  
The test statistic is given by,  
  
${I_n} = \frac{1}{{{n^2}}}\sum\limits_i^{} {\sum\limits_{j \ne i}^{} {{{\hat u}_i}} } {\hat u_j}{K_{\gamma {j_{ij}}}}$
  
where,  
  
${K_{\gamma {j_{ij}}}} = {W_{h,ij}}{L_{\lambda ,ij}}(\gamma  = (h,\lambda ))$,  
  
${\hat u_i} = {y_i} - m({x_i},\hat \beta )$ is the parametric null model's residual,  
  
$\beta$ is a $\sqrt(n)$-consistent estimator of $\beta$ under $H_0$.  
  
Hsiao et al. [-@Hsiao:2007uk] advocate for the use of cross-validation methods for selecting the kernel smoothing paramater vectors, which is the approach we took in this analysis. Under assumptions (A1)-(A3) of thier paper we can obtain the cross-validation based (CV-based) test with the new test statistic ${\hat J_n}$:  
  
$n{({\hat h_1},...,{\hat h_q})^{1/2}}{\hat I_n} \to N(0,\Omega )$ in distribution under $H_0$,  
  
where,  
  
$\Omega  = 2E\left[ {{\sigma ^4}({x_i})f({x_i})} \right]\left[ {\int {{W^2}(v)dv} } \right]$  
  
A consistent estimator of $\Omega$ is given by,  
  
$\hat \Omega  = {n^{ - 2}}2({\hat h_1},...,{\hat h_q})\sum\limits_i^{} {\sum\limits_{j \ne i}^{} {\hat u_i^2} } \hat u_j^2W_{\hat h,ij}^2L_{\hat \lambda ,ij}^2$  
  
Which gives the CV-based test statistic as,  
  
${\hat J_n} = n{({\hat h_1},...,{\hat h_q})^{1/2}}{\hat I_n}{\hat \Omega ^{ - 1/2}} \to N(0,1)$ in distribution under $H_0$.  
  
According to the authors, it can be easily shown that the $\hat J_n$ test statistic diverges to $+ \infty$ if $H_0$ is false.  
  
## Bandwidth Selection models  
  
The importance of proper bandwidth selection in nonparametric analysis is reviewed here. However, for a more detailed discussion of the methods and theory used in this paper see the excellent work by the following authors: Cheng et al.[-@Cheng:1997vy], Hall et al.[-@Hall:2007vq], Hurvich et al.[-@Hurvich:1998wi], Li and Racine[-@li2007nonparametric], Racine and Li[-@Racine:2004tua], and Li and Racine[-@Li:2004tn].  
  
The most popular methods for bandwith selection in applied research are data driven, and of the data driven methods Least-Squares Cross Validation (LSCV) and expected Kullback-Leibler cross-validation also know in the literature as Akaike Information Criterion Cross Validation (aic-CV), are the two used in this paper to evaluate the valididty of using nonparametric techniques to uncover demand elasticities.  
  
### Least-Squares Cross Validation (LSCV)  
  
While there are many methods to determine an optimal bandwidth, the most commonly used in the current literature is the cross-validation method of LSCV which is defined as:  
  
\begin{equation}
LSCV(h) = {\sum\limits_{i = 1}^n {\left[ {{y_i} - {{\hat m}_{ - 1}} - ({{\text{x}}_i})} \right]} ^2}
\end{equation}
  
where,  
  
${\hat m_{ - 1}} - ({{\text{x}}_i})$ is the leave-one-out estimator defined by Li and Racine[-@Li:2004tn] as,  
  
\begin{equation}
{\hat m_{ - 1}} - ({{\text{x}}_i}) = \frac{{\sum\limits_{\scriptstyle j \ne i \hfill \atop 
  \scriptstyle j = 1 \hfill} ^n {{y_h}({{\text{x}}_i},{{\text{x}}_j})} }}{{\sum\limits_{\scriptstyle j \ne i \hfill \atop 
  \scriptstyle j = 1 \hfill} ^n {{k_h}({{\text{x}}_i},{{\text{x}}_j})} }}
\end{equation}  
  
An examination of equation 2 reveals that our algorithm will require $n^2$ calculations. Several authors have suggested a derevation of the LSCV that side-steps this issue; we have decided to use the classic formuala as defined in the original paper by Li and Racine[-@Li:2004tn]. It should be noted however, as the data sample grows, the calculations become computationally burdensome.  
  
Gaining in popularity is a cross-validation bandwidth selection method developed by Hurvich et al.[-@Hurvich:1998wi] called Akaike Information Criterion Cross Validation (aic-CV). This method is similar to Kullback-Leibler information criterion method which is the foundation for density estimation using likelihood cross-validation. However, the true benefit of using the aic-CV method is the property of reduced variability the tendancy for this method to undersmooth.  
  
### Akaike Information Criterion Cross Validation (aic-CV)  
  
The aic-CV method is defined as:  
  
\begin{equation}
AI{C_c}(h) = \ln ({\hat \sigma ^2}) + \frac{{1 + p/n}}{{1 - (p + 2)/n}} = \ln ({\hat \sigma ^2}) + 1 + \frac{{2(p + 1)}}{{n - p - 2}}
\end{equation}
  
where,  
  
$AIC_C$ is the bias corrected version,  
  
$\hat \sigma^2$ is the estimated error variance,  
  
p is the number of regression (or autoregressive) paramaters in the model.  
  
which gives the following formula for smoothing paramter selction:  
  
\begin{equation}
AI{C_c}(h) = \ln ({\hat \sigma ^2}) + \frac{{1 + tr(H)/n}}{{1 - \{ tr(H) + 2\} /n}} = \ln ({\hat \sigma ^2}) + 1 + \frac{{2(tr(H) + 1)}}{{n - tr(H) - 2}}
\end{equation}
  
where,  
  
${\hat \sigma ^2} = {n^{ - 1}}{\sum\limits_{i = 1}^n {\left[ {{y_i} - \hat m({{\text{x}}_i})} \right]} ^2}$,  
  
and H is called the hat matrix or smoother matrix defined by:  
  
\[H = \left[ {\begin{array}{*{20}{c}}
  {\frac{{K({{\text{x}}_1}{\text{,}}{{\text{x}}_1})}}{{\hat f({{\text{x}}_1})}}}&{\frac{{K({{\text{x}}_1}{\text{,}}{{\text{x}}_1})}}{{\hat f({{\text{x}}_1})}}}& \vdots &{\frac{{K({{\text{x}}_1}{\text{,}}{{\text{x}}_n})}}{{\hat f({{\text{x}}_1})}}} \\ 
  {\frac{{K({{\text{x}}_2}{\text{,}}{{\text{x}}_1})}}{{\hat f({{\text{x}}_2})}}}&{\frac{{K({{\text{x}}_2}{\text{,}}{{\text{x}}_2})}}{{\hat f({{\text{x}}_2})}}}& \vdots &{\frac{{K({{\text{x}}_2}{\text{,}}{{\text{x}}_n})}}{{\hat f({{\text{x}}_2})}}} \\ 
   \vdots & \vdots & \ddots & \vdots  \\ 
  {\frac{{K({{\text{x}}_n}{\text{,}}{{\text{x}}_1})}}{{\hat f({{\text{x}}_n})}}}&{\frac{{K({{\text{x}}_n}{\text{,}}{{\text{x}}_2})}}{{\hat f({{\text{x}}_n})}}}& \cdots &{\frac{{K({{\text{x}}_n}{\text{,}}{{\text{x}}_n})}}{{\hat f({{\text{x}}_n})}}} 
\end{array}} \right]\]
  
The trace of the H matrix should be thought of as a measure of the number of paramaters in the nonparametric model [@Hastie:1993vz].  
  
## Nonparametric Regression models  
  
While the method of bandwidth selection is critical, the end game of this analysis is to use the bandwidth estimator in a regression analysis to evaluate a comparison between elasticities derived from a paramertic model estimated under Least Squares versus elasticities derived from nonparamtertic regression methods. With this goal in mind we now explore nonparamteric regression methods.  
  
### Local-Constant Least Squares (LCLS)  
  
The LCLS regression was the first nonparametric regression model developed and is sometimes refered to as the Nadaraya–Watson estimator after Nadaraya [-@Nadaraya:1964de] and Watson [-@Watson:1964vz]. This method minimizes the kernel weighted least-squres regression of $y_i$ on a constant $c$. If we assume some function $g$ s.t. $y_i=g(x_i)+\epsilon_i$ then our objective with LCLS is to approximate $g(\cdot)$ with a constant $c$. More formally the LCLS can be defined as:  
  
\begin{equation}
LCLS(y;{\text{x}}) = \frac{1}{{n{h_y}|{\text{h}}|}}\sum\limits_{i = 1}^n k \left( {\frac{{{y_i} - y}}{{{h_y}}}} \right){K_h}({{\text{x}}_i}{\text{,x}})
\end{equation}
  
where,  
  
${K_h}({{\text{x}}_i}{\text{,x}})$ is the product kernel.

\begin{equation}
a = \arg \min \left\{ {\hat m({\text{x}})} \right\} = \frac{{\sum\limits_{i = 1}^n {{K_h}({{\text{x}}_i}{\text{,x}}){y_i}} }}{{\sum\limits_{i = 1}^n {{K_h}({{\text{x}}_i}{\text{,x}})} }}
\end{equation}
  
where $a$ can be thought of as our constant $c$ which approximates $g(\cdot)$.  
  
The above estimator can be represented in matrix form as:  
  
$$\hat m({\text{x}}) = {(\textbf{i}'K({\text{x}})\textbf{i})^{ - 1}}\textbf{i}'K({\text{x}})y$$
  
where,  
  
$\textbf{i}$ is the identity matrix of dimension $n \times 1$,  
  
and $K({\text{x}})$ is the same product kernel from equation 6,  
  
$$K({\text{x}}) = \left[ {\begin{array}{*{20}{c}}
  {{K_h}({{\text{x}}_1}{\text{,x}})}&0&0&0 \\ 
  0&{{K_h}({{\text{x}}_2}{\text{,x}})}&0&0 \\ 
  0&0& \ddots &0 \\ 
  0&0&0&{{K_h}({{\text{x}}_n}{\text{,x}})} 
\end{array}} \right]$$
  
One of the problems with the LCLS estimator is that it is bias. This may not be of concern depending on the circumstances but it should be noted.



### Local-Linear Least Squares (LLLS)  
  
The LLLS estimator has been growing in popularity as an alternative to the LCLS method. In addition to increasing use in applied research the LLLS estimator has been dominated the theoretical literature.  
  
Conceptually, the LLLS estimator is similar to the LCLS with the exception that instead of fitting a constant to our left hand side variable we now consider fitting a locally defined line segment. This allows us to more accuratly model the data and is especially useful when we have outliers and nonlinear relationships in our data.  
  
We can think of our local-linear fitting as a local Taylor Series expansion at some point \textbf{x}. In other words, for the relationship $y=m(x)+u$, we have paired data on $(y_1,x_1), (y_2,x_2), ..., (y_n,x_n)$. This allows us to take a local Taylor approximation for each of the points \textbf{x}. It now become clear, with locally fitting with kernel weighting, problematic data (outliers, nonlinearity) can be betetr modeled by LLLS.  
  
The LLLS estimator as developed and defined by Li and Racine [-@Li:2004tn] is given as follows:  
  
We begin with a Taylor expansion of \textbf{x} for every observation $i$ as,  
  
$${y_i} = m({x_i}) + {u_i} \approx m(x) + ({x_i} - x)\beta (x) + {u_i}$$  
  
where,  
  
$(x_i-x)$ is a $1 \times q$ vector and $\beta(x)$ is the gradient vector of dimension $q$.  
  
then,  
  
$${y_i} = \alpha  + ({x_i} - x)b + {u_i} = \left[ {\begin{array}{*{20}{c}}
  1&{({x_i} - x)} 
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
  a \\ 
  b 
\end{array}} \right] + {u_i}$$  
  
where,  
  
$m(x)$ and $\beta(x)$ are paramaters.  
  
Our minimization problem then becomes,  
  
$$\arg \mathop {\min }\limits_{a,b} {\sum\limits_{i = 1}^n {\left[ {{y_i} - a - ({x_i} - x)b} \right]} ^2}{K_h}({x_i},x)$$  
and in matrix notation,  
  
$$\arg \mathop {\min }\limits_\delta  {\left[ {y - X\delta } \right]^\prime }K(x)\left[ {y - X\delta } \right]$$  
  
where,  
  
$\delta=(a,b)^{\prime}$,  
  
$X$ is a $n \times (q+1)$ matrix with the $i^{th}$ row equal $\left[ {\begin{array}{*{20}{c}}
  1&{({x_i} - x)} 
\end{array}} \right]$,  
  
$K(x)$ is $n \times n$ diagonal matrix with the $i^{th}$ element equal to $K_h(x_i,x)$.  
  
If $K(x)$ is an identity matrix then we simply have the OLS estimator.  
  
If $K(x)$ is equal to $\Omega=vcov(\theta)$ then the above relationship reduces to the generalized least squares.  
  
When we minimize the objective function $\hat \delta(x)$ we get:  
  
\begin{equation*}
\begin{split}
\hat \delta (x) & = \left[ {\begin{array}{*{20}{c}}
  {\hat m(x)} \\ 
  {\hat \beta (x)} 
\end{array}} \right] \\ 
  &  = {\left[ {\sum\limits_{i = 1}^n {{K_h}({x_i},x)\left( {\begin{array}{*{20}{c}}
  1 \\ 
  {{x_i} - x} 
\end{array}} \right)\left( {\begin{array}{*{20}{c}}
  1&{({x_i} - x)} 
\end{array}} \right)} } \right]^{ - 1}}\sum\limits_{i = 1}^n {{K_h}({x_i},x)\left( {\begin{array}{*{20}{c}}
  1 \\ 
  {{x_i} - x} 
\end{array}} \right){y_i}}  \\
&  = {\left[ {{X^\prime }K(x)X} \right]^{ - 1}}{X^\prime }K(x)y \\
\end{split}
\end{equation*}
  
where the final equation in matrix form gives our LLLS estimators.  
  
The bias of the LLLS estimator is less complicated to work with than the LCLS and deals better with nonlinear and irregular data. For these reasons many researchers employ the LLLS estimator over the LCLS. However, there are situations where the LCLS estimator will be prefered so an analysis of both methods within the context of the problem is always wise.  
  

  
  
  
  
  
  
## Methods  
  
## Data  
  
```{r, echo=FALSE, message=FALSE, results='asis', cache=TRUE}

stargazer(Dta_sub, header=F, type="latex", summary = TRUE, font.size = "small", notes= c("good 1 = meats", "good 2 = dairy", "good 3 = beans"), notes.align= "l", flip = F, float = T, float.env = "table", title="Summary Statistics")

```
  
# Results and Discussion  
  
```{r, echo=FALSE, message=FALSE, results='asis', cache=TRUE}

stargazer(exx_par_wr2, header=F, type="latex", summary = FALSE, font.size = "small", notes= c("***Significant at the 1 percent level,", "**Significant at the 5 percent level,", "*Significant at the 10 percent level."), notes.align= "r", flip = F, float = T, float.env = "table", title="Parametric - Double Log Demand Model")

stargazer(exx_np_wr2, header=F, type="latex", summary = FALSE, font.size = "small", notes= c("***Significant at the 1 percent level,", "**Significant at the 5 percent level,", "*Significant at the 10 percent level."), notes.align= "r", flip = F, float = T, float.env = "table", title="Nonparametric Regression using Gaussian Kernel")

stargazer(AIDS_elas_M_income_constraint_t_sig_wr2, header=F, type="latex", summary = FALSE, font.size = "small", notes= c("***Significant at the 1 percent level,", "**Significant at the 5 percent level,", "*Significant at the 10 percent level."), notes.align= "r", flip = F, float = T, float.env = "table", title="Full AIDS - Marshallian")

```
  
```{r, echo=FALSE, message=FALSE, results='asis', fig.height=3, cache=TRUE}

par(mfrow=c(1,3))
plot(density(model_1.np$grad[,1]), sub = "Meats - own price elasticity", main= "", xlim=c(-5,5))
plot(density(model_2.np$grad[,2]), sub = "Dairy - own price elasticity", main= "",xlim=c(-5,5))
plot(density(model_3.np$grad[,3]), sub = "Beans - own price elasticity", main= "", xlim=c(-5,5))

plot(density(model_1.np$grad[,4]), sub = "Meats - income elasticity", main= "", xlim=c(-5,5))
plot(density(model_2.np$grad[,4]), sub = "Dairy - income elasticity", main= "",xlim=c(-5,5))
plot(density(model_3.np$grad[,4]), sub = "Beans - income elasticity", main= "", xlim=c(-5,5))

```

# Conclusions
  





\newpage

# References  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
